{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "import re\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData = pd.read_csv('../datasets/nela10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>published_utc</th>\n",
       "      <th>collection_utc</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abcnews--2019-01-30--Who is Stacey Abrams the ...</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>abcnews</td>\n",
       "      <td>Who is Stacey Abrams, the Democrat who will re...</td>\n",
       "      <td>Last November, Stacey Abrams, a Democrat who r...</td>\n",
       "      <td>Cheyenne Haslett</td>\n",
       "      <td>https://abcnews.go.com/Politics/stacey-abrams-...</td>\n",
       "      <td>2019-01-30 19:10:47+00:00</td>\n",
       "      <td>1548893447</td>\n",
       "      <td>1567550233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abcnews--2019-01-30--Ban government shutdowns ...</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>abcnews</td>\n",
       "      <td>Ban government shutdowns? Some Republicans and...</td>\n",
       "      <td>As lawmakers scramble to negotiate a border se...</td>\n",
       "      <td>Benjamin Siegel,\\nTrish Turner\\n</td>\n",
       "      <td>https://abcnews.go.com/Politics/ban-government...</td>\n",
       "      <td>2019-01-30 17:29:04+00:00</td>\n",
       "      <td>1548887344</td>\n",
       "      <td>1567550233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abcnews--2019-01-30--Democrats renew push for ...</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>abcnews</td>\n",
       "      <td>Democrats renew push for equal pay for equal work</td>\n",
       "      <td>Ten years after President Barack Obama signed ...</td>\n",
       "      <td>John Parkinson</td>\n",
       "      <td>https://abcnews.go.com/Politics/dems-renew-pus...</td>\n",
       "      <td>2019-01-30 21:35:45+00:00</td>\n",
       "      <td>1548902145</td>\n",
       "      <td>1567550233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abcnews--2019-01-30--Trump attacks intel chief...</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>abcnews</td>\n",
       "      <td>Trump attacks intel chiefs after they contradi...</td>\n",
       "      <td>President Donald Trump on Wednesday declared \"...</td>\n",
       "      <td>Meridith Mcgraw</td>\n",
       "      <td>https://abcnews.go.com/Politics/trump-attacks-...</td>\n",
       "      <td>2019-01-30 17:03:40+00:00</td>\n",
       "      <td>1548885820</td>\n",
       "      <td>1567550233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abcnews--2019-01-30--Border security battle be...</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>abcnews</td>\n",
       "      <td>Border security battle begins as Congress grap...</td>\n",
       "      <td>Republicans and Democrats on the exclusive com...</td>\n",
       "      <td>John Parkinson,\\nBenjamin Siegel\\n</td>\n",
       "      <td>https://abcnews.go.com/Politics/border-securit...</td>\n",
       "      <td>2019-01-30 15:09:47+00:00</td>\n",
       "      <td>1548878987</td>\n",
       "      <td>1567550233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57152</th>\n",
       "      <td>zerohedge--2019-11-01--Hong Kong Officer Faces...</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>zerohedge</td>\n",
       "      <td>Hong Kong Officer Faces Death Threats After Fi...</td>\n",
       "      <td>It's unclear exactly when it happened (the BBG...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>http://feedproxy.google.com/~r/zerohedge/feed/...</td>\n",
       "      <td>Fri, 01 Nov 2019 22:45:00 +0000</td>\n",
       "      <td>1572662700</td>\n",
       "      <td>1572648625</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57153</th>\n",
       "      <td>zerohedge--2019-11-01--Enough \"Quid Pro Quo\" G...</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>zerohedge</td>\n",
       "      <td>Enough \"Quid Pro Quo\" Gaslighting!</td>\n",
       "      <td>Horse trading is the oxygen of politics; it is...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>http://feedproxy.google.com/~r/zerohedge/feed/...</td>\n",
       "      <td>Fri, 01 Nov 2019 22:25:00 +0000</td>\n",
       "      <td>1572661500</td>\n",
       "      <td>1572648625</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57154</th>\n",
       "      <td>zerohedge--2019-11-01--\"Born For This? I Don't...</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>zerohedge</td>\n",
       "      <td>\"Born For This? I Don't Think So\" - Trump Mock...</td>\n",
       "      <td>Having plunged from over 10% to just 1% in the...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>http://feedproxy.google.com/~r/zerohedge/feed/...</td>\n",
       "      <td>Fri, 01 Nov 2019 21:56:35 +0000</td>\n",
       "      <td>1572659795</td>\n",
       "      <td>1572648626</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57155</th>\n",
       "      <td>zerohedge--2019-11-01--How Iran Used Google To...</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>zerohedge</td>\n",
       "      <td>How Iran Used Google To Disrupt 5% Of Global O...</td>\n",
       "      <td>Officials at Saudi Aramco believe that Iran us...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>http://feedproxy.google.com/~r/zerohedge/feed/...</td>\n",
       "      <td>Fri, 01 Nov 2019 21:45:00 +0000</td>\n",
       "      <td>1572659100</td>\n",
       "      <td>1572648627</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57156</th>\n",
       "      <td>zerohedge--2019-11-01--Malaysia Rejects Goldma...</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>zerohedge</td>\n",
       "      <td>Malaysia Rejects Goldman's Offer Of \"Less Than...</td>\n",
       "      <td>In an interview with the Financial Times, Mala...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>http://feedproxy.google.com/~r/zerohedge/feed/...</td>\n",
       "      <td>Fri, 01 Nov 2019 21:25:00 +0000</td>\n",
       "      <td>1572657900</td>\n",
       "      <td>1572648625</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57157 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id        date  \\\n",
       "0      abcnews--2019-01-30--Who is Stacey Abrams the ...  2019-01-30   \n",
       "1      abcnews--2019-01-30--Ban government shutdowns ...  2019-01-30   \n",
       "2      abcnews--2019-01-30--Democrats renew push for ...  2019-01-30   \n",
       "3      abcnews--2019-01-30--Trump attacks intel chief...  2019-01-30   \n",
       "4      abcnews--2019-01-30--Border security battle be...  2019-01-30   \n",
       "...                                                  ...         ...   \n",
       "57152  zerohedge--2019-11-01--Hong Kong Officer Faces...  2019-11-01   \n",
       "57153  zerohedge--2019-11-01--Enough \"Quid Pro Quo\" G...  2019-11-01   \n",
       "57154  zerohedge--2019-11-01--\"Born For This? I Don't...  2019-11-01   \n",
       "57155  zerohedge--2019-11-01--How Iran Used Google To...  2019-11-01   \n",
       "57156  zerohedge--2019-11-01--Malaysia Rejects Goldma...  2019-11-01   \n",
       "\n",
       "          source                                              title  \\\n",
       "0        abcnews  Who is Stacey Abrams, the Democrat who will re...   \n",
       "1        abcnews  Ban government shutdowns? Some Republicans and...   \n",
       "2        abcnews  Democrats renew push for equal pay for equal work   \n",
       "3        abcnews  Trump attacks intel chiefs after they contradi...   \n",
       "4        abcnews  Border security battle begins as Congress grap...   \n",
       "...          ...                                                ...   \n",
       "57152  zerohedge  Hong Kong Officer Faces Death Threats After Fi...   \n",
       "57153  zerohedge                 Enough \"Quid Pro Quo\" Gaslighting!   \n",
       "57154  zerohedge  \"Born For This? I Don't Think So\" - Trump Mock...   \n",
       "57155  zerohedge  How Iran Used Google To Disrupt 5% Of Global O...   \n",
       "57156  zerohedge  Malaysia Rejects Goldman's Offer Of \"Less Than...   \n",
       "\n",
       "                                                 content  \\\n",
       "0      Last November, Stacey Abrams, a Democrat who r...   \n",
       "1      As lawmakers scramble to negotiate a border se...   \n",
       "2      Ten years after President Barack Obama signed ...   \n",
       "3      President Donald Trump on Wednesday declared \"...   \n",
       "4      Republicans and Democrats on the exclusive com...   \n",
       "...                                                  ...   \n",
       "57152  It's unclear exactly when it happened (the BBG...   \n",
       "57153  Horse trading is the oxygen of politics; it is...   \n",
       "57154  Having plunged from over 10% to just 1% in the...   \n",
       "57155  Officials at Saudi Aramco believe that Iran us...   \n",
       "57156  In an interview with the Financial Times, Mala...   \n",
       "\n",
       "                                   author  \\\n",
       "0                        Cheyenne Haslett   \n",
       "1        Benjamin Siegel,\\nTrish Turner\\n   \n",
       "2                          John Parkinson   \n",
       "3                         Meridith Mcgraw   \n",
       "4      John Parkinson,\\nBenjamin Siegel\\n   \n",
       "...                                   ...   \n",
       "57152                        Tyler Durden   \n",
       "57153                        Tyler Durden   \n",
       "57154                        Tyler Durden   \n",
       "57155                        Tyler Durden   \n",
       "57156                        Tyler Durden   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://abcnews.go.com/Politics/stacey-abrams-...   \n",
       "1      https://abcnews.go.com/Politics/ban-government...   \n",
       "2      https://abcnews.go.com/Politics/dems-renew-pus...   \n",
       "3      https://abcnews.go.com/Politics/trump-attacks-...   \n",
       "4      https://abcnews.go.com/Politics/border-securit...   \n",
       "...                                                  ...   \n",
       "57152  http://feedproxy.google.com/~r/zerohedge/feed/...   \n",
       "57153  http://feedproxy.google.com/~r/zerohedge/feed/...   \n",
       "57154  http://feedproxy.google.com/~r/zerohedge/feed/...   \n",
       "57155  http://feedproxy.google.com/~r/zerohedge/feed/...   \n",
       "57156  http://feedproxy.google.com/~r/zerohedge/feed/...   \n",
       "\n",
       "                             published  published_utc  collection_utc  \\\n",
       "0            2019-01-30 19:10:47+00:00     1548893447      1567550233   \n",
       "1            2019-01-30 17:29:04+00:00     1548887344      1567550233   \n",
       "2            2019-01-30 21:35:45+00:00     1548902145      1567550233   \n",
       "3            2019-01-30 17:03:40+00:00     1548885820      1567550233   \n",
       "4            2019-01-30 15:09:47+00:00     1548878987      1567550233   \n",
       "...                                ...            ...             ...   \n",
       "57152  Fri, 01 Nov 2019 22:45:00 +0000     1572662700      1572648625   \n",
       "57153  Fri, 01 Nov 2019 22:25:00 +0000     1572661500      1572648625   \n",
       "57154  Fri, 01 Nov 2019 21:56:35 +0000     1572659795      1572648626   \n",
       "57155  Fri, 01 Nov 2019 21:45:00 +0000     1572659100      1572648627   \n",
       "57156  Fri, 01 Nov 2019 21:25:00 +0000     1572657900      1572648625   \n",
       "\n",
       "       Reliability  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "57152            2  \n",
       "57153            2  \n",
       "57154            2  \n",
       "57155            2  \n",
       "57156            2  \n",
       "\n",
       "[57157 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only content, Reliability as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only content, Reliability as columns\n",
    "totalData = totalData.drop(['id','date','source','title','author','url','published','published_utc','collection_utc'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last November, Stacey Abrams, a Democrat who r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As lawmakers scramble to negotiate a border se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten years after President Barack Obama signed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Donald Trump on Wednesday declared \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republicans and Democrats on the exclusive com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57152</th>\n",
       "      <td>It's unclear exactly when it happened (the BBG...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57153</th>\n",
       "      <td>Horse trading is the oxygen of politics; it is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57154</th>\n",
       "      <td>Having plunged from over 10% to just 1% in the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57155</th>\n",
       "      <td>Officials at Saudi Aramco believe that Iran us...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57156</th>\n",
       "      <td>In an interview with the Financial Times, Mala...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57157 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  Reliability\n",
       "0      Last November, Stacey Abrams, a Democrat who r...            0\n",
       "1      As lawmakers scramble to negotiate a border se...            0\n",
       "2      Ten years after President Barack Obama signed ...            0\n",
       "3      President Donald Trump on Wednesday declared \"...            0\n",
       "4      Republicans and Democrats on the exclusive com...            0\n",
       "...                                                  ...          ...\n",
       "57152  It's unclear exactly when it happened (the BBG...            2\n",
       "57153  Horse trading is the oxygen of politics; it is...            2\n",
       "57154  Having plunged from over 10% to just 1% in the...            2\n",
       "57155  Officials at Saudi Aramco believe that Iran us...            2\n",
       "57156  In an interview with the Financial Times, Mala...            2\n",
       "\n",
       "[57157 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out unique Reliability labels type\n",
    "totalData.Reliability.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop off mixed reliability(label=1) type\n",
    "totalData = totalData[totalData.Reliability != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out unique Reliability labels type\n",
    "totalData.Reliability.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData = totalData.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "totalData.content = totalData.content.apply(clean_text)\n",
    "totalData.content = totalData.content.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34010    one afternoon in the summer of  bob gramling d...\n",
       "25620    why isnt jason clarke more famous hes very goo...\n",
       "14889    tender buttons has closed its doors and we hav...\n",
       "53331    rahaf alqunun is not alonethe only thing more ...\n",
       "30527    britain is bracing for the coldest night of th...\n",
       "1137     singapore a pristine white statue of a man in ...\n",
       "2944     three teenagers have been arrested after a yea...\n",
       "4658     the partialgovernment shutdownhas glided into ...\n",
       "18744    gordon sondland the us ambassador to the europ...\n",
       "7751     four out of five people identified as suspects...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalData.content.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(totalData['content'], totalData['Reliability'], test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Unigram and Bigram Tf-Idf feature vectors from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=None)\n",
    "tfidf_vect.fit(totalData['content'])\n",
    "xtrain_tfidf_unigram =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf_unigram =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# bigram level tf-idf (bigram in this case)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=None)\n",
    "tfidf_vect_ngram.fit(totalData['content'])\n",
    "xtrain_tfidf_bigram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_bigram =  tfidf_vect_ngram.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,  feature_vector_valid, valid_y,is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #print(\"In Validation Data\",metrics.accuracy_score(predictions, valid_y))\n",
    "    print(\"f1 score: \",f1_score(valid_y,predictions,average='weighted'))        \n",
    "    return metrics.accuracy_score(valid_y,predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Line Model Used\n",
    "## 1. Naive Bayes\n",
    "## 2. Linear Classifier\n",
    "## 3. Bagging\n",
    "## 4. Boosting\n",
    "## 5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Unigram Tf-IDF features vectors using Naive Bayes\n",
      "f1 score:  0.6949164043381264\n",
      "Accuracy:  0.7856892932120364\n",
      "For Bigram Tf-IDF features vectors using Naive Bayes\n",
      "f1 score:  0.707853317795271\n",
      "Accuracy:  0.7913750874737578\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "print(\"For Unigram Tf-IDF features vectors using Naive Bayes\")\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram, valid_y)\n",
    "print (\"Accuracy: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "print(\"For Bigram Tf-IDF features vectors using Naive Bayes\")\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram, valid_y)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Unigram TF-Idf feature vectors using Logistic Regression\n",
      "f1 score:  0.8778954704561857\n",
      "accuracy:  0.887071378586424\n",
      "For Biigram TF-Idf feature vectors using Logistic Regression\n",
      "f1 score:  0.8258199257551274\n",
      "Accuracy:  0.85444366689993\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "print(\"For Unigram TF-Idf feature vectors using Logistic Regression\")\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram, valid_y)\n",
    "print(\"accuracy: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "print(\"For Biigram TF-Idf feature vectors using Logistic Regression\")\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500),  xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram, valid_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Unigram Tf-Idf feature vectors using Random Forest Classifier\n",
      "f1 score:  0.7889982224045017\n",
      "Accuracy:  0.8326627011896431\n",
      "For Bigram Tf-Idf feature vectors using Random Forest Classifier\n",
      "f1 score:  0.819699814467292\n",
      "Accuracy:  0.8511196641007698\n"
     ]
    }
   ],
   "source": [
    "# RF on Word Level TF IDF Vectors\n",
    "print(\"For Unigram Tf-Idf feature vectors using Random Forest Classifier\")\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram, valid_y)\n",
    "print (\"Accuracy: \", accuracy)\n",
    "\n",
    "# RF on ngram Level TF IDF Vectors\n",
    "print(\"For Bigram Tf-Idf feature vectors using Random Forest Classifier\")\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram, valid_y)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Unigram Tf-Idf feature vectors using Extreme Gradient Boosting\n",
      "f1 score:  0.8889110797087082\n",
      "Accuracy:  0.8964310706787963\n",
      "For Bigram Tf-Idf feature vectors using Extreme Gradient Boosting\n",
      "f1 score:  0.8775083946497313\n",
      "Accuracy:  0.8889958012596221\n"
     ]
    }
   ],
   "source": [
    "print(\"For Unigram Tf-Idf feature vectors using Extreme Gradient Boosting\")\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_unigram.tocsc(), train_y, xvalid_tfidf_unigram.tocsc(), valid_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on ngram Level TF IDF Vectors\n",
    "print(\"For Bigram Tf-Idf feature vectors using Extreme Gradient Boosting\")\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_bigram.tocsc(), train_y, xvalid_tfidf_bigram.tocsc(), valid_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Unigram Tf-Idf feature vectors using SVM\n",
      "f1 score:  0.9001589355399741\n",
      "Accuracy:  0.9065780265920224\n",
      "For Bigram Tf-Idf feature vectors using SVM\n",
      "f1 score:  0.84919268919277\n",
      "Accuracy:  0.8704513645906228\n"
     ]
    }
   ],
   "source": [
    "print(\"For Unigram Tf-Idf feature vectors using SVM\")\n",
    "#SVM Model on Unigram TF-IDF\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_unigram.tocsc(), train_y, xvalid_tfidf_unigram.tocsc(), valid_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# SVM Model on Bigram TF-IDF\n",
    "print(\"For Bigram Tf-Idf feature vectors using SVM\")\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_bigram.tocsc(), train_y, xvalid_tfidf_bigram.tocsc(), valid_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
