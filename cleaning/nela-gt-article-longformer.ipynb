{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:46.545602Z",
     "iopub.status.busy": "2020-10-29T14:39:46.545339Z",
     "iopub.status.idle": "2020-10-29T14:39:49.717773Z",
     "shell.execute_reply": "2020-10-29T14:39:49.717135Z",
     "shell.execute_reply.started": "2020-10-29T14:39:46.545559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:49.722297Z",
     "iopub.status.busy": "2020-10-29T14:39:49.721965Z",
     "iopub.status.idle": "2020-10-29T14:39:52.622094Z",
     "shell.execute_reply": "2020-10-29T14:39:52.621472Z",
     "shell.execute_reply.started": "2020-10-29T14:39:49.722250Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['bert-base-uncased', 'distilbert-base-uncased-finetuned-sst-2-english', 'roberta-large', 'monologg/electra-small-finetuned-imdb', 'xlnet-base-cased', 'xlm-roberta-large']\n",
    "model_num = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num], do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:52.623322Z",
     "iopub.status.busy": "2020-10-29T14:39:52.623136Z",
     "iopub.status.idle": "2020-10-29T14:39:52.649936Z",
     "shell.execute_reply": "2020-10-29T14:39:52.649302Z",
     "shell.execute_reply.started": "2020-10-29T14:39:52.623284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:52.650945Z",
     "iopub.status.busy": "2020-10-29T14:39:52.650791Z",
     "iopub.status.idle": "2020-10-29T14:39:52.863301Z",
     "shell.execute_reply": "2020-10-29T14:39:52.862814Z",
     "shell.execute_reply.started": "2020-10-29T14:39:52.650924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 29 20:09:52 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   30C    P5    13W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 50%   83C    P2   237W / 250W |   7459MiB / 11178MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     9W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8     8W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    1     24032      C   ....raha/miniconda3/envs/fastai/bin/python  7449MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:52.864408Z",
     "iopub.status.busy": "2020-10-29T14:39:52.864229Z",
     "iopub.status.idle": "2020-10-29T14:39:52.867584Z",
     "shell.execute_reply": "2020-10-29T14:39:52.866995Z",
     "shell.execute_reply.started": "2020-10-29T14:39:52.864385Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '../datasets/nela-gt/nela10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:52.868633Z",
     "iopub.status.busy": "2020-10-29T14:39:52.868418Z",
     "iopub.status.idle": "2020-10-29T14:39:54.526520Z",
     "shell.execute_reply": "2020-10-29T14:39:54.525953Z",
     "shell.execute_reply.started": "2020-10-29T14:39:52.868611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 57,157\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>published_utc</th>\n",
       "      <th>collection_utc</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31270</th>\n",
       "      <td>theindependent--2019-02-04--Fracking must be b...</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>theindependent</td>\n",
       "      <td>Fracking must be banned unless government rela...</td>\n",
       "      <td>Britain’s richest man has demanded that the go...</td>\n",
       "      <td>Ben Chapman</td>\n",
       "      <td>http://www.independent.co.uk/news/business/new...</td>\n",
       "      <td>2019-02-04 13:55:00+00:00</td>\n",
       "      <td>1549306500</td>\n",
       "      <td>1567549661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9661</th>\n",
       "      <td>eveningstandard--2019-06-27--Tory leadership l...</td>\n",
       "      <td>2019-06-27</td>\n",
       "      <td>eveningstandard</td>\n",
       "      <td>Tory leadership latest: Boris Johnson says it ...</td>\n",
       "      <td>Boris Johnson has said it would be \"absolutely...</td>\n",
       "      <td>Bonnie Christian</td>\n",
       "      <td>https://www.standard.co.uk/news/politics/boris...</td>\n",
       "      <td>2019-06-27 19:51:00+00:00</td>\n",
       "      <td>1561679460</td>\n",
       "      <td>1567537832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27540</th>\n",
       "      <td>theguardianuk--2019-03-19--Mueller suspected T...</td>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>theguardianuk</td>\n",
       "      <td>Mueller suspected Trump lawyer may have been a...</td>\n",
       "      <td>Robert Mueller persuaded a judge within weeks ...</td>\n",
       "      <td>Jon Swaine in New York</td>\n",
       "      <td>https://www.theguardian.com/us-news/2019/mar/1...</td>\n",
       "      <td>2019-03-19 15:40:56+00:00</td>\n",
       "      <td>1553024456</td>\n",
       "      <td>1567545679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>theindependent--2019-07-06--Pride in London Sa...</td>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>theindependent</td>\n",
       "      <td>Pride in London: Sadiq Khan attacks Boris John...</td>\n",
       "      <td>The mayor of London, Sadiq Khan, has attacked ...</td>\n",
       "      <td>Adam Forrest</td>\n",
       "      <td>https://www.independent.co.uk/news/uk/politics...</td>\n",
       "      <td>2019-07-06 16:36:45+00:00</td>\n",
       "      <td>1562445405</td>\n",
       "      <td>1567536639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35454</th>\n",
       "      <td>theirishtimes--2019-03-19--Economist Paul Krug...</td>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>theirishtimes</td>\n",
       "      <td>Economist Paul Krugman to participate in Kilke...</td>\n",
       "      <td>The Nobel Prize winning economist and New York...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.irishtimes.com/news/ireland/irish-...</td>\n",
       "      <td>2019-03-19 21:03:31+00:00</td>\n",
       "      <td>1553043811</td>\n",
       "      <td>1567545671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19703</th>\n",
       "      <td>slate--2019-09-10--Did the Cynicism of Reality...</td>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>slate</td>\n",
       "      <td>Did the Cynicism of Reality Television Lead to...</td>\n",
       "      <td>On The Gist, Bolton is out.\\n\\nIn the intervie...</td>\n",
       "      <td>Mike Pesca</td>\n",
       "      <td>https://slate.com/podcasts/the-gist/2019/09/ne...</td>\n",
       "      <td>2019-09-10 23:39:37+00:00</td>\n",
       "      <td>1568173177</td>\n",
       "      <td>1569330499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21430</th>\n",
       "      <td>tass--2019-05-26--Medvedev congratulates Russi...</td>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>tass</td>\n",
       "      <td>Medvedev congratulates Russian team on winning...</td>\n",
       "      <td>© Alexander Astafyev/Russian Government Press ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://tass.com/sport/1060145</td>\n",
       "      <td>2019-05-26 23:10:47+00:00</td>\n",
       "      <td>1558926647</td>\n",
       "      <td>1567540160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17577</th>\n",
       "      <td>realclearpolitics--2019-02-14--For 2020 Democr...</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>realclearpolitics</td>\n",
       "      <td>For 2020, Democrats Are Lookin' for Somebody t...</td>\n",
       "      <td>Valentine's Day is here, and Democrats want to...</td>\n",
       "      <td>&lt;a href=\"/authors/bill_schneider\" data-mce-hre...</td>\n",
       "      <td>https://www.realclearpolitics.com/2019/02/14/f...</td>\n",
       "      <td>2019-02-14 20:41:24+00:00</td>\n",
       "      <td>1550194884</td>\n",
       "      <td>1567548487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16541</th>\n",
       "      <td>politicalwire--2019-09-10--Story Around John B...</td>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>politicalwire</td>\n",
       "      <td>Story Around John Bolton’s Ouster Doesn’t Add Up</td>\n",
       "      <td>Aaron Blake: “Just an hour before the announce...</td>\n",
       "      <td>Taegan Goddard</td>\n",
       "      <td>https://politicalwire.com/2019/09/10/story-aro...</td>\n",
       "      <td>2019-09-10 17:36:25+00:00</td>\n",
       "      <td>1568151385</td>\n",
       "      <td>1569330623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38316</th>\n",
       "      <td>thetelegraph--2019-01-10--Donald Trump says 10...</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>thetelegraph</td>\n",
       "      <td>Donald Trump says '100pc' he will declare nati...</td>\n",
       "      <td>Donald Trump has said “100 per cent” he will d...</td>\n",
       "      <td>Ben Riley-Smith</td>\n",
       "      <td>https://www.telegraph.co.uk/news/2019/01/10/do...</td>\n",
       "      <td>2019-01-10 16:09:10+00:00</td>\n",
       "      <td>1547154550</td>\n",
       "      <td>1567553090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id        date  \\\n",
       "31270  theindependent--2019-02-04--Fracking must be b...  2019-02-04   \n",
       "9661   eveningstandard--2019-06-27--Tory leadership l...  2019-06-27   \n",
       "27540  theguardianuk--2019-03-19--Mueller suspected T...  2019-03-19   \n",
       "30893  theindependent--2019-07-06--Pride in London Sa...  2019-07-06   \n",
       "35454  theirishtimes--2019-03-19--Economist Paul Krug...  2019-03-19   \n",
       "19703  slate--2019-09-10--Did the Cynicism of Reality...  2019-09-10   \n",
       "21430  tass--2019-05-26--Medvedev congratulates Russi...  2019-05-26   \n",
       "17577  realclearpolitics--2019-02-14--For 2020 Democr...  2019-02-14   \n",
       "16541  politicalwire--2019-09-10--Story Around John B...  2019-09-10   \n",
       "38316  thetelegraph--2019-01-10--Donald Trump says 10...  2019-01-10   \n",
       "\n",
       "                  source                                              title  \\\n",
       "31270     theindependent  Fracking must be banned unless government rela...   \n",
       "9661     eveningstandard  Tory leadership latest: Boris Johnson says it ...   \n",
       "27540      theguardianuk  Mueller suspected Trump lawyer may have been a...   \n",
       "30893     theindependent  Pride in London: Sadiq Khan attacks Boris John...   \n",
       "35454      theirishtimes  Economist Paul Krugman to participate in Kilke...   \n",
       "19703              slate  Did the Cynicism of Reality Television Lead to...   \n",
       "21430               tass  Medvedev congratulates Russian team on winning...   \n",
       "17577  realclearpolitics  For 2020, Democrats Are Lookin' for Somebody t...   \n",
       "16541      politicalwire   Story Around John Bolton’s Ouster Doesn’t Add Up   \n",
       "38316       thetelegraph  Donald Trump says '100pc' he will declare nati...   \n",
       "\n",
       "                                                 content  \\\n",
       "31270  Britain’s richest man has demanded that the go...   \n",
       "9661   Boris Johnson has said it would be \"absolutely...   \n",
       "27540  Robert Mueller persuaded a judge within weeks ...   \n",
       "30893  The mayor of London, Sadiq Khan, has attacked ...   \n",
       "35454  The Nobel Prize winning economist and New York...   \n",
       "19703  On The Gist, Bolton is out.\\n\\nIn the intervie...   \n",
       "21430  © Alexander Astafyev/Russian Government Press ...   \n",
       "17577  Valentine's Day is here, and Democrats want to...   \n",
       "16541  Aaron Blake: “Just an hour before the announce...   \n",
       "38316  Donald Trump has said “100 per cent” he will d...   \n",
       "\n",
       "                                                  author  \\\n",
       "31270                                        Ben Chapman   \n",
       "9661                                    Bonnie Christian   \n",
       "27540                             Jon Swaine in New York   \n",
       "30893                                       Adam Forrest   \n",
       "35454                                                NaN   \n",
       "19703                                         Mike Pesca   \n",
       "21430                                                NaN   \n",
       "17577  <a href=\"/authors/bill_schneider\" data-mce-hre...   \n",
       "16541                                     Taegan Goddard   \n",
       "38316                                    Ben Riley-Smith   \n",
       "\n",
       "                                                     url  \\\n",
       "31270  http://www.independent.co.uk/news/business/new...   \n",
       "9661   https://www.standard.co.uk/news/politics/boris...   \n",
       "27540  https://www.theguardian.com/us-news/2019/mar/1...   \n",
       "30893  https://www.independent.co.uk/news/uk/politics...   \n",
       "35454  https://www.irishtimes.com/news/ireland/irish-...   \n",
       "19703  https://slate.com/podcasts/the-gist/2019/09/ne...   \n",
       "21430                      http://tass.com/sport/1060145   \n",
       "17577  https://www.realclearpolitics.com/2019/02/14/f...   \n",
       "16541  https://politicalwire.com/2019/09/10/story-aro...   \n",
       "38316  https://www.telegraph.co.uk/news/2019/01/10/do...   \n",
       "\n",
       "                       published  published_utc  collection_utc  Reliability  \n",
       "31270  2019-02-04 13:55:00+00:00     1549306500      1567549661            0  \n",
       "9661   2019-06-27 19:51:00+00:00     1561679460      1567537832            0  \n",
       "27540  2019-03-19 15:40:56+00:00     1553024456      1567545679            0  \n",
       "30893  2019-07-06 16:36:45+00:00     1562445405      1567536639            0  \n",
       "35454  2019-03-19 21:03:31+00:00     1553043811      1567545671            0  \n",
       "19703  2019-09-10 23:39:37+00:00     1568173177      1569330499            0  \n",
       "21430  2019-05-26 23:10:47+00:00     1558926647      1567540160            0  \n",
       "17577  2019-02-14 20:41:24+00:00     1550194884      1567548487            0  \n",
       "16541  2019-09-10 17:36:25+00:00     1568151385      1569330623            0  \n",
       "38316  2019-01-10 16:09:10+00:00     1547154550      1567553090            0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:54.527583Z",
     "iopub.status.busy": "2020-10-29T14:39:54.527406Z",
     "iopub.status.idle": "2020-10-29T14:39:54.530383Z",
     "shell.execute_reply": "2020-10-29T14:39:54.529813Z",
     "shell.execute_reply.started": "2020-10-29T14:39:54.527562Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:54.532196Z",
     "iopub.status.busy": "2020-10-29T14:39:54.532046Z",
     "iopub.status.idle": "2020-10-29T14:39:54.536110Z",
     "shell.execute_reply": "2020-10-29T14:39:54.535596Z",
     "shell.execute_reply.started": "2020-10-29T14:39:54.532177Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_rel(num):\n",
    "    if num == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:39:54.537378Z",
     "iopub.status.busy": "2020-10-29T14:39:54.537222Z",
     "iopub.status.idle": "2020-10-29T14:40:06.236640Z",
     "shell.execute_reply": "2020-10-29T14:40:06.235585Z",
     "shell.execute_reply.started": "2020-10-29T14:39:54.537359Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "df['content'] = df['content'].str.replace('\\d+', '')\n",
    "df['Reliability'] = df['Reliability'].apply(change_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:06.238360Z",
     "iopub.status.busy": "2020-10-29T14:40:06.238167Z",
     "iopub.status.idle": "2020-10-29T14:40:18.357885Z",
     "shell.execute_reply": "2020-10-29T14:40:18.357312Z",
     "shell.execute_reply.started": "2020-10-29T14:40:06.238329Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_split(text1):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(text1.split())//150 >0:\n",
    "        n = len(text1.split())//150\n",
    "    else: \n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text1.split()[:200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return l_total\n",
    "df['content_split'] = df['content'].apply(get_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:18.358848Z",
     "iopub.status.busy": "2020-10-29T14:40:18.358668Z",
     "iopub.status.idle": "2020-10-29T14:40:19.573485Z",
     "shell.execute_reply": "2020-10-29T14:40:19.572963Z",
     "shell.execute_reply.started": "2020-10-29T14:40:18.358819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15183"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].apply(count_words).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.574548Z",
     "iopub.status.busy": "2020-10-29T14:40:19.574402Z",
     "iopub.status.idle": "2020-10-29T14:40:19.598181Z",
     "shell.execute_reply": "2020-10-29T14:40:19.597723Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.574528Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['content_split'], df['Reliability'], test_size=0.2, stratify=df['Reliability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.598978Z",
     "iopub.status.busy": "2020-10-29T14:40:19.598837Z",
     "iopub.status.idle": "2020-10-29T14:40:19.651844Z",
     "shell.execute_reply": "2020-10-29T14:40:19.651289Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.598959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139010, 139010)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l = []\n",
    "label_l = []\n",
    "index_l =[]\n",
    "for row,label in zip(train_x, train_y):\n",
    "    for l in row:\n",
    "        train_l.append(l)\n",
    "        label_l.append(label)\n",
    "len(train_l), len(label_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.652739Z",
     "iopub.status.busy": "2020-10-29T14:40:19.652576Z",
     "iopub.status.idle": "2020-10-29T14:40:19.669264Z",
     "shell.execute_reply": "2020-10-29T14:40:19.668777Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.652720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35074, 35074)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for row,label in zip(valid_x, valid_y):\n",
    "    for l in row:\n",
    "        val_l.append(l)\n",
    "        val_label_l.append(label)\n",
    "len(val_l), len(val_label_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.669965Z",
     "iopub.status.busy": "2020-10-29T14:40:19.669826Z",
     "iopub.status.idle": "2020-10-29T14:40:19.672499Z",
     "shell.execute_reply": "2020-10-29T14:40:19.672056Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.669947Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 220\n",
    "posts = train_l\n",
    "categories = label_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.673371Z",
     "iopub.status.busy": "2020-10-29T14:40:19.673211Z",
     "iopub.status.idle": "2020-10-29T14:40:19.691520Z",
     "shell.execute_reply": "2020-10-29T14:40:19.691072Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.673353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 14746,   553,    30,    10,  4439,   114,   447,    19,  2354,\n",
       "          5189, 10710,   197,    28,  1687,    10,  1846,  6255,   268,    26,\n",
       "            14,   358,   881,  9455,    14,    47,    70,  3594,  1415,    13,\n",
       "             8, 10593,    13,   335,    14, 47764, 47471,    21,  1976,   217,\n",
       "          2207,    15,    24,   757,    45,  2542,     9,  5670,   259,   447,\n",
       "            19, 47764, 47471,    11,   143,  2148,    53,   939,   109,   216,\n",
       "            14,   358,  1736,    14,  3372,    10,   433,  9455,   259,  1415,\n",
       "            13,    14,   335,   144,     9,    47,   431,    15,    14,   335,\n",
       "            98,   939,   206,    47,   241,    95,    25,  8943,    25,  1268,\n",
       "          1493,    11,    14,   609,  6255,   268,  1143,   627,   864,   376,\n",
       "            11,  6203,     7,  6704,  4401,   876, 14204,  4544, 10861, 19395,\n",
       "            15, 28087,   857,    14,   447,    19,     5, 10710,   197,    28,\n",
       "          1687,    10,  1846,    39,  5835,   908,    15,   481,  1228,   829,\n",
       "           410,  4627,   124,    31,     5, 46773,  6516,   433,    54,  2026,\n",
       "             7,    28,  2190,     7,  4538,    10,   481,  1228, 39224, 47471,\n",
       "            16,    65,     9,     5,   129,   538, 14419,    54,    34,   393,\n",
       "            56,     7,   696,    10, 14921,    50,    10,  5494, 22870,    13,\n",
       "         10467,  4486,    50, 12030,   340,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        posts[0],                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:40:19.692342Z",
     "iopub.status.busy": "2020-10-29T14:40:19.692199Z",
     "iopub.status.idle": "2020-10-29T14:44:34.151455Z",
     "shell.execute_reply": "2020-10-29T14:44:34.150819Z",
     "shell.execute_reply.started": "2020-10-29T14:40:19.692324Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    try:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            truncation=True,\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "    except:\n",
    "        print(sent)\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:34.153239Z",
     "iopub.status.busy": "2020-10-29T14:44:34.153077Z",
     "iopub.status.idle": "2020-10-29T14:44:34.174473Z",
     "shell.execute_reply": "2020-10-29T14:44:34.173808Z",
     "shell.execute_reply.started": "2020-10-29T14:44:34.153219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121,633 training samples\n",
      "17,377 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.875 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:34.175928Z",
     "iopub.status.busy": "2020-10-29T14:44:34.175589Z",
     "iopub.status.idle": "2020-10-29T14:44:34.180666Z",
     "shell.execute_reply": "2020-10-29T14:44:34.180044Z",
     "shell.execute_reply.started": "2020-10-29T14:44:34.175892Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:34.182130Z",
     "iopub.status.busy": "2020-10-29T14:44:34.181716Z",
     "iopub.status.idle": "2020-10-29T14:44:53.106344Z",
     "shell.execute_reply": "2020-10-29T14:44:53.105514Z",
     "shell.execute_reply.started": "2020-10-29T14:44:34.182086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "import torch.nn as nn\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "# model = nn.DataParallel(model)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.107697Z",
     "iopub.status.busy": "2020-10-29T14:44:53.107536Z",
     "iopub.status.idle": "2020-10-29T14:44:53.117913Z",
     "shell.execute_reply": "2020-10-29T14:44:53.117358Z",
     "shell.execute_reply.started": "2020-10-29T14:44:53.107675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 395 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50265, 1024)\n",
      "roberta.embeddings.position_embeddings.weight            (514, 1024)\n",
      "roberta.embeddings.token_type_embeddings.weight            (1, 1024)\n",
      "roberta.embeddings.LayerNorm.weight                          (1024,)\n",
      "roberta.embeddings.LayerNorm.bias                            (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.query.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight       (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.key.bias              (1024,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.value.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight   (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias          (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight      (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias      (1024,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight       (4096, 1024)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (4096,)\n",
      "roberta.encoder.layer.0.output.dense.weight             (1024, 4096)\n",
      "roberta.encoder.layer.0.output.dense.bias                    (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight              (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                 (1024, 1024)\n",
      "classifier.dense.bias                                        (1024,)\n",
      "classifier.out_proj.weight                                 (2, 1024)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.118813Z",
     "iopub.status.busy": "2020-10-29T14:44:53.118656Z",
     "iopub.status.idle": "2020-10-29T14:44:53.123529Z",
     "shell.execute_reply": "2020-10-29T14:44:53.122861Z",
     "shell.execute_reply.started": "2020-10-29T14:44:53.118788Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.124279Z",
     "iopub.status.busy": "2020-10-29T14:44:53.124094Z",
     "iopub.status.idle": "2020-10-29T14:44:53.130556Z",
     "shell.execute_reply": "2020-10-29T14:44:53.130091Z",
     "shell.execute_reply.started": "2020-10-29T14:44:53.124260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.131313Z",
     "iopub.status.busy": "2020-10-29T14:44:53.131173Z",
     "iopub.status.idle": "2020-10-29T14:44:53.136965Z",
     "shell.execute_reply": "2020-10-29T14:44:53.136462Z",
     "shell.execute_reply.started": "2020-10-29T14:44:53.131295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.137724Z",
     "iopub.status.busy": "2020-10-29T14:44:53.137587Z",
     "iopub.status.idle": "2020-10-29T14:44:53.143631Z",
     "shell.execute_reply": "2020-10-29T14:44:53.143046Z",
     "shell.execute_reply.started": "2020-10-29T14:44:53.137705Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:44:53.145727Z",
     "iopub.status.busy": "2020-10-29T14:44:53.145585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  15,205.    Elapsed: 0:00:22.\n",
      "  Batch    80  of  15,205.    Elapsed: 0:00:45.\n",
      "  Batch   120  of  15,205.    Elapsed: 0:01:08.\n",
      "  Batch   160  of  15,205.    Elapsed: 0:01:31.\n",
      "  Batch   200  of  15,205.    Elapsed: 0:01:53.\n",
      "  Batch   240  of  15,205.    Elapsed: 0:02:17.\n",
      "  Batch   280  of  15,205.    Elapsed: 0:02:40.\n",
      "  Batch   320  of  15,205.    Elapsed: 0:03:04.\n",
      "  Batch   360  of  15,205.    Elapsed: 0:03:28.\n",
      "  Batch   400  of  15,205.    Elapsed: 0:03:52.\n",
      "  Batch   440  of  15,205.    Elapsed: 0:04:16.\n",
      "  Batch   480  of  15,205.    Elapsed: 0:04:40.\n",
      "  Batch   520  of  15,205.    Elapsed: 0:05:04.\n",
      "  Batch   560  of  15,205.    Elapsed: 0:05:27.\n",
      "  Batch   600  of  15,205.    Elapsed: 0:05:51.\n",
      "  Batch   640  of  15,205.    Elapsed: 0:06:15.\n",
      "  Batch   680  of  15,205.    Elapsed: 0:06:39.\n",
      "  Batch   720  of  15,205.    Elapsed: 0:07:03.\n",
      "  Batch   760  of  15,205.    Elapsed: 0:07:27.\n",
      "  Batch   800  of  15,205.    Elapsed: 0:07:51.\n",
      "  Batch   840  of  15,205.    Elapsed: 0:08:15.\n",
      "  Batch   880  of  15,205.    Elapsed: 0:08:38.\n",
      "  Batch   920  of  15,205.    Elapsed: 0:09:02.\n",
      "  Batch   960  of  15,205.    Elapsed: 0:09:26.\n",
      "  Batch 1,000  of  15,205.    Elapsed: 0:09:49.\n",
      "  Batch 1,040  of  15,205.    Elapsed: 0:10:13.\n",
      "  Batch 1,080  of  15,205.    Elapsed: 0:10:37.\n",
      "  Batch 1,120  of  15,205.    Elapsed: 0:11:00.\n",
      "  Batch 1,160  of  15,205.    Elapsed: 0:11:24.\n",
      "  Batch 1,200  of  15,205.    Elapsed: 0:11:48.\n",
      "  Batch 1,240  of  15,205.    Elapsed: 0:12:12.\n",
      "  Batch 1,280  of  15,205.    Elapsed: 0:12:35.\n",
      "  Batch 1,320  of  15,205.    Elapsed: 0:12:59.\n",
      "  Batch 1,360  of  15,205.    Elapsed: 0:13:23.\n",
      "  Batch 1,400  of  15,205.    Elapsed: 0:13:46.\n",
      "  Batch 1,440  of  15,205.    Elapsed: 0:14:10.\n",
      "  Batch 1,480  of  15,205.    Elapsed: 0:14:34.\n",
      "  Batch 1,520  of  15,205.    Elapsed: 0:14:57.\n",
      "  Batch 1,560  of  15,205.    Elapsed: 0:15:21.\n",
      "  Batch 1,600  of  15,205.    Elapsed: 0:15:45.\n",
      "  Batch 1,640  of  15,205.    Elapsed: 0:16:08.\n",
      "  Batch 1,680  of  15,205.    Elapsed: 0:16:32.\n",
      "  Batch 1,720  of  15,205.    Elapsed: 0:16:56.\n",
      "  Batch 1,760  of  15,205.    Elapsed: 0:17:20.\n",
      "  Batch 1,800  of  15,205.    Elapsed: 0:17:43.\n",
      "  Batch 1,840  of  15,205.    Elapsed: 0:18:07.\n",
      "  Batch 1,880  of  15,205.    Elapsed: 0:18:31.\n",
      "  Batch 1,920  of  15,205.    Elapsed: 0:18:54.\n",
      "  Batch 1,960  of  15,205.    Elapsed: 0:19:18.\n",
      "  Batch 2,000  of  15,205.    Elapsed: 0:19:42.\n",
      "  Batch 2,040  of  15,205.    Elapsed: 0:20:06.\n",
      "  Batch 2,080  of  15,205.    Elapsed: 0:20:29.\n",
      "  Batch 2,120  of  15,205.    Elapsed: 0:20:53.\n",
      "  Batch 2,160  of  15,205.    Elapsed: 0:21:17.\n",
      "  Batch 2,200  of  15,205.    Elapsed: 0:21:41.\n",
      "  Batch 2,240  of  15,205.    Elapsed: 0:22:05.\n",
      "  Batch 2,280  of  15,205.    Elapsed: 0:22:29.\n",
      "  Batch 2,320  of  15,205.    Elapsed: 0:22:53.\n",
      "  Batch 2,360  of  15,205.    Elapsed: 0:23:16.\n",
      "  Batch 2,400  of  15,205.    Elapsed: 0:23:40.\n",
      "  Batch 2,440  of  15,205.    Elapsed: 0:24:04.\n",
      "  Batch 2,480  of  15,205.    Elapsed: 0:24:28.\n",
      "  Batch 2,520  of  15,205.    Elapsed: 0:24:51.\n",
      "  Batch 2,560  of  15,205.    Elapsed: 0:25:15.\n",
      "  Batch 2,600  of  15,205.    Elapsed: 0:25:39.\n",
      "  Batch 2,640  of  15,205.    Elapsed: 0:26:03.\n",
      "  Batch 2,680  of  15,205.    Elapsed: 0:26:26.\n",
      "  Batch 2,720  of  15,205.    Elapsed: 0:26:50.\n",
      "  Batch 2,760  of  15,205.    Elapsed: 0:27:14.\n",
      "  Batch 2,800  of  15,205.    Elapsed: 0:27:38.\n",
      "  Batch 2,840  of  15,205.    Elapsed: 0:28:01.\n",
      "  Batch 2,880  of  15,205.    Elapsed: 0:28:25.\n",
      "  Batch 2,920  of  15,205.    Elapsed: 0:28:49.\n",
      "  Batch 2,960  of  15,205.    Elapsed: 0:29:13.\n",
      "  Batch 3,000  of  15,205.    Elapsed: 0:29:36.\n",
      "  Batch 3,040  of  15,205.    Elapsed: 0:30:00.\n",
      "  Batch 3,080  of  15,205.    Elapsed: 0:30:24.\n",
      "  Batch 3,120  of  15,205.    Elapsed: 0:30:48.\n",
      "  Batch 3,160  of  15,205.    Elapsed: 0:31:12.\n",
      "  Batch 3,200  of  15,205.    Elapsed: 0:31:35.\n",
      "  Batch 3,240  of  15,205.    Elapsed: 0:31:59.\n",
      "  Batch 3,280  of  15,205.    Elapsed: 0:32:23.\n",
      "  Batch 3,320  of  15,205.    Elapsed: 0:32:47.\n",
      "  Batch 3,360  of  15,205.    Elapsed: 0:33:10.\n",
      "  Batch 3,400  of  15,205.    Elapsed: 0:33:34.\n",
      "  Batch 3,440  of  15,205.    Elapsed: 0:33:58.\n",
      "  Batch 3,480  of  15,205.    Elapsed: 0:34:22.\n",
      "  Batch 3,520  of  15,205.    Elapsed: 0:34:45.\n",
      "  Batch 3,560  of  15,205.    Elapsed: 0:35:09.\n",
      "  Batch 3,600  of  15,205.    Elapsed: 0:35:33.\n",
      "  Batch 3,640  of  15,205.    Elapsed: 0:35:56.\n",
      "  Batch 3,680  of  15,205.    Elapsed: 0:36:20.\n",
      "  Batch 3,720  of  15,205.    Elapsed: 0:36:44.\n",
      "  Batch 3,760  of  15,205.    Elapsed: 0:37:08.\n",
      "  Batch 3,800  of  15,205.    Elapsed: 0:37:31.\n",
      "  Batch 3,840  of  15,205.    Elapsed: 0:37:55.\n",
      "  Batch 3,880  of  15,205.    Elapsed: 0:38:19.\n",
      "  Batch 3,920  of  15,205.    Elapsed: 0:38:43.\n",
      "  Batch 3,960  of  15,205.    Elapsed: 0:39:06.\n",
      "  Batch 4,000  of  15,205.    Elapsed: 0:39:30.\n",
      "  Batch 4,040  of  15,205.    Elapsed: 0:39:54.\n",
      "  Batch 4,080  of  15,205.    Elapsed: 0:40:18.\n",
      "  Batch 4,120  of  15,205.    Elapsed: 0:40:42.\n",
      "  Batch 4,160  of  15,205.    Elapsed: 0:41:06.\n",
      "  Batch 4,200  of  15,205.    Elapsed: 0:41:29.\n",
      "  Batch 4,240  of  15,205.    Elapsed: 0:41:53.\n",
      "  Batch 4,280  of  15,205.    Elapsed: 0:42:17.\n",
      "  Batch 4,320  of  15,205.    Elapsed: 0:42:41.\n",
      "  Batch 4,360  of  15,205.    Elapsed: 0:43:05.\n",
      "  Batch 4,400  of  15,205.    Elapsed: 0:43:28.\n",
      "  Batch 4,440  of  15,205.    Elapsed: 0:43:52.\n",
      "  Batch 4,480  of  15,205.    Elapsed: 0:44:16.\n",
      "  Batch 4,520  of  15,205.    Elapsed: 0:44:40.\n",
      "  Batch 4,560  of  15,205.    Elapsed: 0:45:04.\n",
      "  Batch 4,600  of  15,205.    Elapsed: 0:45:27.\n",
      "  Batch 4,640  of  15,205.    Elapsed: 0:45:51.\n",
      "  Batch 4,680  of  15,205.    Elapsed: 0:46:15.\n",
      "  Batch 4,720  of  15,205.    Elapsed: 0:46:39.\n",
      "  Batch 4,760  of  15,205.    Elapsed: 0:47:03.\n",
      "  Batch 4,800  of  15,205.    Elapsed: 0:47:26.\n",
      "  Batch 4,840  of  15,205.    Elapsed: 0:47:50.\n",
      "  Batch 4,880  of  15,205.    Elapsed: 0:48:14.\n",
      "  Batch 4,920  of  15,205.    Elapsed: 0:48:38.\n",
      "  Batch 4,960  of  15,205.    Elapsed: 0:49:02.\n",
      "  Batch 5,000  of  15,205.    Elapsed: 0:49:25.\n",
      "  Batch 5,040  of  15,205.    Elapsed: 0:49:49.\n",
      "  Batch 5,080  of  15,205.    Elapsed: 0:50:13.\n",
      "  Batch 5,120  of  15,205.    Elapsed: 0:50:37.\n",
      "  Batch 5,160  of  15,205.    Elapsed: 0:51:01.\n",
      "  Batch 5,200  of  15,205.    Elapsed: 0:51:25.\n",
      "  Batch 5,240  of  15,205.    Elapsed: 0:51:49.\n",
      "  Batch 5,280  of  15,205.    Elapsed: 0:52:13.\n",
      "  Batch 5,320  of  15,205.    Elapsed: 0:52:36.\n",
      "  Batch 5,360  of  15,205.    Elapsed: 0:53:00.\n",
      "  Batch 5,400  of  15,205.    Elapsed: 0:53:24.\n",
      "  Batch 5,440  of  15,205.    Elapsed: 0:53:48.\n",
      "  Batch 5,480  of  15,205.    Elapsed: 0:54:12.\n",
      "  Batch 5,520  of  15,205.    Elapsed: 0:54:35.\n",
      "  Batch 5,560  of  15,205.    Elapsed: 0:54:59.\n",
      "  Batch 5,600  of  15,205.    Elapsed: 0:55:23.\n",
      "  Batch 5,640  of  15,205.    Elapsed: 0:55:47.\n",
      "  Batch 5,680  of  15,205.    Elapsed: 0:56:11.\n",
      "  Batch 5,720  of  15,205.    Elapsed: 0:56:34.\n",
      "  Batch 5,760  of  15,205.    Elapsed: 0:56:58.\n",
      "  Batch 5,800  of  15,205.    Elapsed: 0:57:22.\n",
      "  Batch 5,840  of  15,205.    Elapsed: 0:57:46.\n",
      "  Batch 5,880  of  15,205.    Elapsed: 0:58:10.\n",
      "  Batch 5,920  of  15,205.    Elapsed: 0:58:34.\n",
      "  Batch 5,960  of  15,205.    Elapsed: 0:58:58.\n",
      "  Batch 6,000  of  15,205.    Elapsed: 0:59:22.\n",
      "  Batch 6,040  of  15,205.    Elapsed: 0:59:45.\n",
      "  Batch 6,080  of  15,205.    Elapsed: 1:00:09.\n",
      "  Batch 6,120  of  15,205.    Elapsed: 1:00:33.\n",
      "  Batch 6,160  of  15,205.    Elapsed: 1:00:57.\n",
      "  Batch 6,200  of  15,205.    Elapsed: 1:01:21.\n",
      "  Batch 6,240  of  15,205.    Elapsed: 1:01:45.\n",
      "  Batch 6,280  of  15,205.    Elapsed: 1:02:09.\n",
      "  Batch 6,320  of  15,205.    Elapsed: 1:02:32.\n",
      "  Batch 6,360  of  15,205.    Elapsed: 1:02:56.\n",
      "  Batch 6,400  of  15,205.    Elapsed: 1:03:20.\n",
      "  Batch 6,440  of  15,205.    Elapsed: 1:03:44.\n",
      "  Batch 6,480  of  15,205.    Elapsed: 1:04:08.\n",
      "  Batch 6,520  of  15,205.    Elapsed: 1:04:31.\n",
      "  Batch 6,560  of  15,205.    Elapsed: 1:04:55.\n",
      "  Batch 6,600  of  15,205.    Elapsed: 1:05:19.\n",
      "  Batch 6,640  of  15,205.    Elapsed: 1:05:43.\n",
      "  Batch 6,680  of  15,205.    Elapsed: 1:06:07.\n",
      "  Batch 6,720  of  15,205.    Elapsed: 1:06:31.\n",
      "  Batch 6,760  of  15,205.    Elapsed: 1:06:55.\n",
      "  Batch 6,800  of  15,205.    Elapsed: 1:07:19.\n",
      "  Batch 6,840  of  15,205.    Elapsed: 1:07:43.\n",
      "  Batch 6,880  of  15,205.    Elapsed: 1:08:07.\n",
      "  Batch 6,920  of  15,205.    Elapsed: 1:08:31.\n",
      "  Batch 6,960  of  15,205.    Elapsed: 1:08:55.\n",
      "  Batch 7,000  of  15,205.    Elapsed: 1:09:19.\n",
      "  Batch 7,040  of  15,205.    Elapsed: 1:09:43.\n",
      "  Batch 7,080  of  15,205.    Elapsed: 1:10:07.\n",
      "  Batch 7,120  of  15,205.    Elapsed: 1:10:30.\n",
      "  Batch 7,160  of  15,205.    Elapsed: 1:10:54.\n",
      "  Batch 7,200  of  15,205.    Elapsed: 1:11:18.\n",
      "  Batch 7,240  of  15,205.    Elapsed: 1:11:42.\n",
      "  Batch 7,280  of  15,205.    Elapsed: 1:12:06.\n",
      "  Batch 7,320  of  15,205.    Elapsed: 1:12:30.\n",
      "  Batch 7,360  of  15,205.    Elapsed: 1:12:54.\n",
      "  Batch 7,400  of  15,205.    Elapsed: 1:13:18.\n",
      "  Batch 7,440  of  15,205.    Elapsed: 1:13:42.\n",
      "  Batch 7,480  of  15,205.    Elapsed: 1:14:05.\n",
      "  Batch 7,520  of  15,205.    Elapsed: 1:14:29.\n",
      "  Batch 7,560  of  15,205.    Elapsed: 1:14:53.\n",
      "  Batch 7,600  of  15,205.    Elapsed: 1:15:17.\n",
      "  Batch 7,640  of  15,205.    Elapsed: 1:15:41.\n",
      "  Batch 7,680  of  15,205.    Elapsed: 1:16:05.\n",
      "  Batch 7,720  of  15,205.    Elapsed: 1:16:29.\n",
      "  Batch 7,760  of  15,205.    Elapsed: 1:16:53.\n",
      "  Batch 7,800  of  15,205.    Elapsed: 1:17:17.\n",
      "  Batch 7,840  of  15,205.    Elapsed: 1:17:41.\n",
      "  Batch 7,880  of  15,205.    Elapsed: 1:18:05.\n",
      "  Batch 7,920  of  15,205.    Elapsed: 1:18:29.\n",
      "  Batch 7,960  of  15,205.    Elapsed: 1:18:52.\n",
      "  Batch 8,000  of  15,205.    Elapsed: 1:19:16.\n",
      "  Batch 8,040  of  15,205.    Elapsed: 1:19:40.\n",
      "  Batch 8,080  of  15,205.    Elapsed: 1:20:04.\n",
      "  Batch 8,120  of  15,205.    Elapsed: 1:20:28.\n",
      "  Batch 8,160  of  15,205.    Elapsed: 1:20:52.\n",
      "  Batch 8,200  of  15,205.    Elapsed: 1:21:16.\n",
      "  Batch 8,240  of  15,205.    Elapsed: 1:21:40.\n",
      "  Batch 8,280  of  15,205.    Elapsed: 1:22:04.\n",
      "  Batch 8,320  of  15,205.    Elapsed: 1:22:27.\n",
      "  Batch 8,360  of  15,205.    Elapsed: 1:22:51.\n",
      "  Batch 8,400  of  15,205.    Elapsed: 1:23:15.\n",
      "  Batch 8,440  of  15,205.    Elapsed: 1:23:39.\n",
      "  Batch 8,480  of  15,205.    Elapsed: 1:24:03.\n",
      "  Batch 8,520  of  15,205.    Elapsed: 1:24:27.\n",
      "  Batch 8,560  of  15,205.    Elapsed: 1:24:51.\n",
      "  Batch 8,600  of  15,205.    Elapsed: 1:25:15.\n",
      "  Batch 8,640  of  15,205.    Elapsed: 1:25:38.\n",
      "  Batch 8,680  of  15,205.    Elapsed: 1:26:02.\n",
      "  Batch 8,720  of  15,205.    Elapsed: 1:26:26.\n",
      "  Batch 8,760  of  15,205.    Elapsed: 1:26:50.\n",
      "  Batch 8,800  of  15,205.    Elapsed: 1:27:14.\n",
      "  Batch 8,840  of  15,205.    Elapsed: 1:27:38.\n",
      "  Batch 8,880  of  15,205.    Elapsed: 1:28:02.\n",
      "  Batch 8,920  of  15,205.    Elapsed: 1:28:26.\n",
      "  Batch 8,960  of  15,205.    Elapsed: 1:28:50.\n",
      "  Batch 9,000  of  15,205.    Elapsed: 1:29:14.\n",
      "  Batch 9,040  of  15,205.    Elapsed: 1:29:38.\n",
      "  Batch 9,080  of  15,205.    Elapsed: 1:30:02.\n",
      "  Batch 9,120  of  15,205.    Elapsed: 1:30:26.\n",
      "  Batch 9,160  of  15,205.    Elapsed: 1:30:49.\n",
      "  Batch 9,200  of  15,205.    Elapsed: 1:31:13.\n",
      "  Batch 9,240  of  15,205.    Elapsed: 1:31:37.\n",
      "  Batch 9,280  of  15,205.    Elapsed: 1:32:01.\n",
      "  Batch 9,320  of  15,205.    Elapsed: 1:32:25.\n",
      "  Batch 9,360  of  15,205.    Elapsed: 1:32:49.\n",
      "  Batch 9,400  of  15,205.    Elapsed: 1:33:13.\n",
      "  Batch 9,440  of  15,205.    Elapsed: 1:33:36.\n",
      "  Batch 9,480  of  15,205.    Elapsed: 1:34:00.\n",
      "  Batch 9,520  of  15,205.    Elapsed: 1:34:24.\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "#         print(loss)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    if epoch_i >= 1:\n",
    "        inp = input()\n",
    "        if inp.startswith('y'):\n",
    "            break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
