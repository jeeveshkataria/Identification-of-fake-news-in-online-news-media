{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:08:56.371044Z",
     "iopub.status.busy": "2020-10-30T00:08:56.370858Z",
     "iopub.status.idle": "2020-10-30T00:08:58.382698Z",
     "shell.execute_reply": "2020-10-30T00:08:58.381865Z",
     "shell.execute_reply.started": "2020-10-30T00:08:56.370993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:08:58.384512Z",
     "iopub.status.busy": "2020-10-30T00:08:58.384306Z",
     "iopub.status.idle": "2020-10-30T00:09:00.804138Z",
     "shell.execute_reply": "2020-10-30T00:09:00.803570Z",
     "shell.execute_reply.started": "2020-10-30T00:08:58.384476Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['bert-base-multilingual-cased', 'xlm-roberta-base', 'sagorsarker/bangla-bert-base']\n",
    "model_num = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:00.805611Z",
     "iopub.status.busy": "2020-10-30T00:09:00.805401Z",
     "iopub.status.idle": "2020-10-30T00:09:00.833401Z",
     "shell.execute_reply": "2020-10-30T00:09:00.832742Z",
     "shell.execute_reply.started": "2020-10-30T00:09:00.805587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:00.834756Z",
     "iopub.status.busy": "2020-10-30T00:09:00.834533Z",
     "iopub.status.idle": "2020-10-30T00:09:00.841589Z",
     "shell.execute_reply": "2020-10-30T00:09:00.841151Z",
     "shell.execute_reply.started": "2020-10-30T00:09:00.834735Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH= '../datasets/banfake/LabeledAuthentic-7K.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:00.842523Z",
     "iopub.status.busy": "2020-10-30T00:09:00.842338Z",
     "iopub.status.idle": "2020-10-30T00:09:01.126904Z",
     "shell.execute_reply": "2020-10-30T00:09:01.126456Z",
     "shell.execute_reply.started": "2020-10-30T00:09:00.842484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 7,202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>domain</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>3498</td>\n",
       "      <td>risingbd.com</td>\n",
       "      <td>2018-09-22 18:28:16</td>\n",
       "      <td>National</td>\n",
       "      <td>পুলিশ</td>\n",
       "      <td>Related</td>\n",
       "      <td>পেট্রোল বোমাসহ ৫ শিবিরকর্মী গ্রেপ্তার</td>\n",
       "      <td>সিরাজগঞ্জ সংবাদদাতা : নাশকতার পরিকল্পনার অভিযো...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>6311</td>\n",
       "      <td>banglatribune.com</td>\n",
       "      <td>2018-09-19 19:16:05</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>দুদকের জনসংযোগ কর্মকর্তা,দুদকের মহাপরিচালক (প্...</td>\n",
       "      <td>Related</td>\n",
       "      <td>হঠাৎ বিএসটিআই কার্যালয়ে দুদক</td>\n",
       "      <td>বিভিন্ন ভোজ্যতেলের মান সঠিকভাবে নিরীক্ষিত হচ্ছ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>4651</td>\n",
       "      <td>jugantor.com</td>\n",
       "      <td>2018-09-22 11:54:10</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>২২ সেপ্টেম্বর: ইতিহাসে আজকের এই দিনে</td>\n",
       "      <td>আজ ২২ সেপ্টেম্বর ২০১৮, শনিবার। ০৭ আশ্বিন, ১৪২৪...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>7757</td>\n",
       "      <td>jagonews24.com</td>\n",
       "      <td>2018-09-23 00:24:29</td>\n",
       "      <td>National</td>\n",
       "      <td>শরীয়তপুর পৌরসভার ৮নং ওয়ার্ডের কাউন্সিলর</td>\n",
       "      <td>Related</td>\n",
       "      <td>দক্ষিণ আফ্রিকায় ঘোড়ার কবলে পড়ে বাংলাদেশি যুবক ...</td>\n",
       "      <td>দক্ষিণ আফ্রিকায় ঘোড়ার কবলে পড়ে ফারুক হাওলাদার ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>7139</td>\n",
       "      <td>jagonews24.com</td>\n",
       "      <td>2018-09-25 21:47:24</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>যমুনা টিভি ও যুগান্তরের বিরুদ্ধে প্রাণ-আরএফএল ...</td>\n",
       "      <td>মিথ্যা ও মানহানিকর সংবাদ প্রকাশের অভিযোগে যমুন...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>4499</td>\n",
       "      <td>somoynews.tv</td>\n",
       "      <td>2018-09-19 18:18:32</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>উইকেটের জন্য মরিয়া ভারত</td>\n",
       "      <td>বাচা-মরার লড়াইয়ে ভারতের বিপক্ষে কঠিন লড়াই করছে...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>475</td>\n",
       "      <td>dailynayadiganta.com</td>\n",
       "      <td>2018-09-22 13:02:43</td>\n",
       "      <td>Politics</td>\n",
       "      <td>আওয়ামী লীগের সাধারণ সম্পাদক এবং সড়ক পরিবহন ও স...</td>\n",
       "      <td>Related</td>\n",
       "      <td>ঐক্যবদ্ধ থাকলে আগামী নির্বাচনেও বিজয়ী হবো : ওব...</td>\n",
       "      <td>আওয়ামী লীগের সাধারণ সম্পাদক এবং সড়ক পরিবহন ও স...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>709</td>\n",
       "      <td>risingbd.com</td>\n",
       "      <td>2018-09-22 17:16:49</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>মেয়েরা ঠিক যেখানে টিপ পরে</td>\n",
       "      <td>(ভিয়েতনামের পথে: ৪২তম পর্ব) ফেরদৌস জামান: দুই ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>3073</td>\n",
       "      <td>banglatribune.com</td>\n",
       "      <td>2018-09-20 01:14:07</td>\n",
       "      <td>National</td>\n",
       "      <td>Reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>বরিশালে স্ত্রী হত্যায় একজনের যাবজ্জীবন</td>\n",
       "      <td>বরিশালে স্ত্রী হত্যার দায়ে মনোয়ার হোসেন হাওলাদ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>618</td>\n",
       "      <td>jagonews24.com</td>\n",
       "      <td>2018-09-21 16:36:31</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Reporter</td>\n",
       "      <td>Related</td>\n",
       "      <td>কারিনার এই শার্টের দাম কত জানেন?</td>\n",
       "      <td>৩৮-এ বয়সে পা রাখলেন কারিনা কাপুর খান। গতকাল বৃ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      articleID                domain                 date       category  \\\n",
       "3295       3498          risingbd.com  2018-09-22 18:28:16       National   \n",
       "5650       6311     banglatribune.com  2018-09-19 19:16:05  Miscellaneous   \n",
       "4297       4651          jugantor.com  2018-09-22 11:54:10      Editorial   \n",
       "6807       7757        jagonews24.com  2018-09-23 00:24:29       National   \n",
       "6310       7139        jagonews24.com  2018-09-25 21:47:24          Crime   \n",
       "4166       4499          somoynews.tv  2018-09-19 18:18:32         Sports   \n",
       "469         475  dailynayadiganta.com  2018-09-22 13:02:43       Politics   \n",
       "698         709          risingbd.com  2018-09-22 17:16:49  Miscellaneous   \n",
       "2902       3073     banglatribune.com  2018-09-20 01:14:07       National   \n",
       "610         618        jagonews24.com  2018-09-21 16:36:31  Entertainment   \n",
       "\n",
       "                                                 source relation  \\\n",
       "3295                                              পুলিশ  Related   \n",
       "5650  দুদকের জনসংযোগ কর্মকর্তা,দুদকের মহাপরিচালক (প্...  Related   \n",
       "4297                                           reporter  Related   \n",
       "6807            শরীয়তপুর পৌরসভার ৮নং ওয়ার্ডের কাউন্সিলর  Related   \n",
       "6310                                           Reporter  Related   \n",
       "4166                                           Reporter  Related   \n",
       "469   আওয়ামী লীগের সাধারণ সম্পাদক এবং সড়ক পরিবহন ও স...  Related   \n",
       "698                                            reporter  Related   \n",
       "2902                                           Reporter  Related   \n",
       "610                                            Reporter  Related   \n",
       "\n",
       "                                               headline  \\\n",
       "3295              পেট্রোল বোমাসহ ৫ শিবিরকর্মী গ্রেপ্তার   \n",
       "5650                       হঠাৎ বিএসটিআই কার্যালয়ে দুদক   \n",
       "4297               ২২ সেপ্টেম্বর: ইতিহাসে আজকের এই দিনে   \n",
       "6807  দক্ষিণ আফ্রিকায় ঘোড়ার কবলে পড়ে বাংলাদেশি যুবক ...   \n",
       "6310  যমুনা টিভি ও যুগান্তরের বিরুদ্ধে প্রাণ-আরএফএল ...   \n",
       "4166                            উইকেটের জন্য মরিয়া ভারত   \n",
       "469   ঐক্যবদ্ধ থাকলে আগামী নির্বাচনেও বিজয়ী হবো : ওব...   \n",
       "698                           মেয়েরা ঠিক যেখানে টিপ পরে   \n",
       "2902             বরিশালে স্ত্রী হত্যায় একজনের যাবজ্জীবন   \n",
       "610                    কারিনার এই শার্টের দাম কত জানেন?   \n",
       "\n",
       "                                                content  label  \n",
       "3295  সিরাজগঞ্জ সংবাদদাতা : নাশকতার পরিকল্পনার অভিযো...    0.0  \n",
       "5650  বিভিন্ন ভোজ্যতেলের মান সঠিকভাবে নিরীক্ষিত হচ্ছ...    1.0  \n",
       "4297  আজ ২২ সেপ্টেম্বর ২০১৮, শনিবার। ০৭ আশ্বিন, ১৪২৪...    0.0  \n",
       "6807  দক্ষিণ আফ্রিকায় ঘোড়ার কবলে পড়ে ফারুক হাওলাদার ...    1.0  \n",
       "6310  মিথ্যা ও মানহানিকর সংবাদ প্রকাশের অভিযোগে যমুন...    1.0  \n",
       "4166  বাচা-মরার লড়াইয়ে ভারতের বিপক্ষে কঠিন লড়াই করছে...    0.0  \n",
       "469   আওয়ামী লীগের সাধারণ সম্পাদক এবং সড়ক পরিবহন ও স...    0.0  \n",
       "698   (ভিয়েতনামের পথে: ৪২তম পর্ব) ফেরদৌস জামান: দুই ...    0.0  \n",
       "2902  বরিশালে স্ত্রী হত্যার দায়ে মনোয়ার হোসেন হাওলাদ...    1.0  \n",
       "610   ৩৮-এ বয়সে পা রাখলেন কারিনা কাপুর খান। গতকাল বৃ...    0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:01.128003Z",
     "iopub.status.busy": "2020-10-30T00:09:01.127838Z",
     "iopub.status.idle": "2020-10-30T00:09:01.131228Z",
     "shell.execute_reply": "2020-10-30T00:09:01.130618Z",
     "shell.execute_reply.started": "2020-10-30T00:09:01.127981Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None\n",
    "def change_rel(num):\n",
    "    return int(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:01.132271Z",
     "iopub.status.busy": "2020-10-30T00:09:01.132105Z",
     "iopub.status.idle": "2020-10-30T00:09:01.827316Z",
     "shell.execute_reply": "2020-10-30T00:09:01.826616Z",
     "shell.execute_reply.started": "2020-10-30T00:09:01.132251Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "#     text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "df['content'] = df['content'].str.replace('\\d+', '')\n",
    "df['label'] = df['label'].apply(change_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:01.828742Z",
     "iopub.status.busy": "2020-10-30T00:09:01.828507Z",
     "iopub.status.idle": "2020-10-30T00:09:02.536683Z",
     "shell.execute_reply": "2020-10-30T00:09:02.536210Z",
     "shell.execute_reply.started": "2020-10-30T00:09:01.828691Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_split(text1):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(text1.split())//120 >0:\n",
    "        n = len(text1.split())//120\n",
    "    else: \n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text1.split()[:160]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = text1.split()[w*120:w*120 + 160]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return l_total\n",
    "df['content_split'] = df['content'].apply(get_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.538859Z",
     "iopub.status.busy": "2020-10-30T00:09:02.538670Z",
     "iopub.status.idle": "2020-10-30T00:09:02.667268Z",
     "shell.execute_reply": "2020-10-30T00:09:02.666761Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.538822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253.42793668425438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].apply(count_words).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.668731Z",
     "iopub.status.busy": "2020-10-30T00:09:02.668558Z",
     "iopub.status.idle": "2020-10-30T00:09:02.677185Z",
     "shell.execute_reply": "2020-10-30T00:09:02.676702Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.668711Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['content_split'], df['label'], test_size=0.2, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.678275Z",
     "iopub.status.busy": "2020-10-30T00:09:02.678008Z",
     "iopub.status.idle": "2020-10-30T00:09:02.687952Z",
     "shell.execute_reply": "2020-10-30T00:09:02.687484Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.678251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10406, 10406)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l = []\n",
    "label_l = []\n",
    "index_l =[]\n",
    "for row,label in zip(train_x, train_y):\n",
    "    for l in row:\n",
    "        train_l.append(l)\n",
    "        label_l.append(label)\n",
    "len(train_l), len(label_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.688973Z",
     "iopub.status.busy": "2020-10-30T00:09:02.688805Z",
     "iopub.status.idle": "2020-10-30T00:09:02.695165Z",
     "shell.execute_reply": "2020-10-30T00:09:02.694738Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.688952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2621, 2621)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for row,label in zip(valid_x, valid_y):\n",
    "    for l in row:\n",
    "        val_l.append(l)\n",
    "        val_label_l.append(label)\n",
    "len(val_l), len(val_label_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.696150Z",
     "iopub.status.busy": "2020-10-30T00:09:02.695960Z",
     "iopub.status.idle": "2020-10-30T00:09:02.701109Z",
     "shell.execute_reply": "2020-10-30T00:09:02.700608Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.696131Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 220\n",
    "posts = train_l\n",
    "categories = label_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.702029Z",
     "iopub.status.busy": "2020-10-30T00:09:02.701855Z",
     "iopub.status.idle": "2020-10-30T00:09:02.715894Z",
     "shell.execute_reply": "2020-10-30T00:09:02.715449Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.702009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('গেল তিন বছরে ধারাবাহিক টাইগাররা টুর্নামেন্ট জেতার এটা উপযুক্ত সময় মাশরাফী-সাকিবদের বলেছেন সাবেক ভারতীয় পেইসার অজিত আগারকার। তার মতে সামর্থ্য বিবেচনায় বাংলাদেশের বড় কোন ট্রফি জয়ের প্রত্যাশা মোটেও বাড়াবাড়ি নয়। শাহরিয়ার নাফিস আশা দেখছেন বলেছেন খুব কময়ের মধ্যেই টুর্নামেন্ট জিতবে টাইগাররা। বাংলাদেশকে এখন খাটো করে দেখে না ভারত। সবশেষ ওডিআই সিরিজ হারের ক্ষত এখনো পোড়ায়। ওয়ার্ল্ড টি টোয়েন্টি কিংবা নিদাহাস ট্রফিতে টাইগার থাবা থেকে কোনমতে বেঁচেছে টিম ইন্ডিয়া। সাকিব-তামিমরাও খুব কাছে গিয়ে ট্রফি না জেতার আক্ষেপ বয়ে বেড়াচ্ছেন। ভারত-বাংলাদেশ ম্যাচ এখন দর্শক টানে। যারা একসময় নিন্দা করতেন তারাও টাইগারদের সমীহ করতে বাধ্য। সাবেকদের চোখে মাশরাফীর দলটা পরিণত। ধারাবাহিকতায় সময় এসেছে বড় কোন সাফল্যের। ভারতের সাবেক পেসার অজিত আগারকার বলেন ক্রিকেট ক্রেজি একটা নেশন বাংলাদেশকে নিয়ে প্রত্যাশা সবসময় বেশি। ওরা ধারাবাহিকভাবে ভালো করছে এখন একটা বড় টুর্নামেন্ট জিতবে সে আশা করাই যায়। ভালো খেলছে ঠিক আছে দারুন কিছু জয়ও পাচ্ছে কোয়ার্টার ফাইনাল কিংবা সেমিফাইনাল খেলছেন। তবে একটা সময় আসে যখন আপনাকে বড় টুর্নামেন্ট জিততে হবে। যে ধরণের ট্যালেন্ট আছে দলে আর',\n",
       " {'input_ids': tensor([[     0, 130001,  61885, 212543,      6,  33560,   5534, 100564,   9976,\n",
       "          240028,   5534,      6,  65935,  10567,   6155, 105354, 122474,  36775,\n",
       "           61901,  30007,  70739,  20895,  14767,   8377,   5534,  16460,   4091,\n",
       "               9,  16984,  26061,   4822,   5521,  89223, 147655, 162248,  44986,\n",
       "            2730,  62873,   6334, 172548,   4979, 193297,  17891,    125,  20340,\n",
       "           13705,   3495, 151261,  88585,  10413,   7277,  16550,  15845,  78676,\n",
       "           63278,  26200,  33237, 125931,  67720,  67206,    896,  10066,   3397,\n",
       "           14329,  37982, 169584, 108645, 168469, 231825,  39267,    125,  96063,\n",
       "             999, 104308,   4480,  67720,   3458, 116471,  67026,  37353,  89223,\n",
       "           69151,  34297,  48054,  21387,   2730,      6,  65935,  10567,   6155,\n",
       "          105354,      6, 172548,  16550, 240028,   5534,    125,  18566,   2937,\n",
       "           42854,  50442,  34523,   5507, 104157,   4480, 112982,    125,  30511,\n",
       "          207708,   2114,  28555,  87756, 174608, 105869,    896,      6,  11984,\n",
       "            3397, 220219,      6,  87728,  23703,   2801,    125,  79743,  10567,\n",
       "          149680,  34916,      6, 215745,   9755,  20404,  17455,  22268,  30010,\n",
       "           29432,   3458, 125931,  67720,   3495, 240028,      6,  56324,  17455,\n",
       "            6638,  33237,   4198,   3495,  37555,  11291,  92984,  12814,  34916,\n",
       "            4198,  10814,  36358,  49714,    125, 228712,      9,   7163,  30550,\n",
       "            4198,   5534,   4876,  69151,  72237, 103294, 125931,  67720,   4480,\n",
       "          122474,  36775,   4979,  11984,    956,   7498,  10947,  16926,  37555,\n",
       "           23703,  68896,   2145,    125, 112982,      9, 234068, 185618,  42854,\n",
       "               6, 133752,   2784,  98595,  15709,    125, 135544,   8339,   3458,\n",
       "           65724,  22268, 149368,  12173,   2145,  60902,   4876, 240028,   5521,\n",
       "           96755,   4091,  10062,  12173,  10947, 125891,    125, 147655,   5521,\n",
       "          157129,    956,  14767,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        posts[0],                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "posts[0], encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:02.716789Z",
     "iopub.status.busy": "2020-10-30T00:09:02.716636Z",
     "iopub.status.idle": "2020-10-30T00:09:13.517117Z",
     "shell.execute_reply": "2020-10-30T00:09:13.516297Z",
     "shell.execute_reply.started": "2020-10-30T00:09:02.716770Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:13.518409Z",
     "iopub.status.busy": "2020-10-30T00:09:13.518229Z",
     "iopub.status.idle": "2020-10-30T00:09:13.523659Z",
     "shell.execute_reply": "2020-10-30T00:09:13.523024Z",
     "shell.execute_reply.started": "2020-10-30T00:09:13.518387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,105 training samples\n",
      "1,301 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.875 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:13.524598Z",
     "iopub.status.busy": "2020-10-30T00:09:13.524434Z",
     "iopub.status.idle": "2020-10-30T00:09:13.528787Z",
     "shell.execute_reply": "2020-10-30T00:09:13.528195Z",
     "shell.execute_reply.started": "2020-10-30T00:09:13.524577Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:13.529765Z",
     "iopub.status.busy": "2020-10-30T00:09:13.529529Z",
     "iopub.status.idle": "2020-10-30T00:09:33.197248Z",
     "shell.execute_reply": "2020-10-30T00:09:33.196661Z",
     "shell.execute_reply.started": "2020-10-30T00:09:13.529744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.198324Z",
     "iopub.status.busy": "2020-10-30T00:09:33.198141Z",
     "iopub.status.idle": "2020-10-30T00:09:33.206433Z",
     "shell.execute_reply": "2020-10-30T00:09:33.205960Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.198287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 203 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (250002, 768)\n",
      "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
      "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
      "roberta.embeddings.LayerNorm.weight                           (768,)\n",
      "roberta.embeddings.LayerNorm.bias                             (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                   (768, 768)\n",
      "classifier.dense.bias                                         (768,)\n",
      "classifier.out_proj.weight                                  (2, 768)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.207555Z",
     "iopub.status.busy": "2020-10-30T00:09:33.207342Z",
     "iopub.status.idle": "2020-10-30T00:09:33.211378Z",
     "shell.execute_reply": "2020-10-30T00:09:33.210814Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.207533Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.212432Z",
     "iopub.status.busy": "2020-10-30T00:09:33.212223Z",
     "iopub.status.idle": "2020-10-30T00:09:33.218142Z",
     "shell.execute_reply": "2020-10-30T00:09:33.217633Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.212410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.219177Z",
     "iopub.status.busy": "2020-10-30T00:09:33.218916Z",
     "iopub.status.idle": "2020-10-30T00:09:33.225372Z",
     "shell.execute_reply": "2020-10-30T00:09:33.224866Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.219137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.226340Z",
     "iopub.status.busy": "2020-10-30T00:09:33.226154Z",
     "iopub.status.idle": "2020-10-30T00:09:33.232018Z",
     "shell.execute_reply": "2020-10-30T00:09:33.231501Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.226319Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:09:33.233039Z",
     "iopub.status.busy": "2020-10-30T00:09:33.232876Z",
     "iopub.status.idle": "2020-10-30T00:15:02.360900Z",
     "shell.execute_reply": "2020-10-30T00:15:02.360231Z",
     "shell.execute_reply.started": "2020-10-30T00:09:33.233019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    570.    Elapsed: 0:00:10.\n",
      "  Batch    80  of    570.    Elapsed: 0:00:21.\n",
      "  Batch   120  of    570.    Elapsed: 0:00:31.\n",
      "  Batch   160  of    570.    Elapsed: 0:00:41.\n",
      "  Batch   200  of    570.    Elapsed: 0:00:52.\n",
      "  Batch   240  of    570.    Elapsed: 0:01:02.\n",
      "  Batch   280  of    570.    Elapsed: 0:01:13.\n",
      "  Batch   320  of    570.    Elapsed: 0:01:23.\n",
      "  Batch   360  of    570.    Elapsed: 0:01:34.\n",
      "  Batch   400  of    570.    Elapsed: 0:01:44.\n",
      "  Batch   440  of    570.    Elapsed: 0:01:55.\n",
      "  Batch   480  of    570.    Elapsed: 0:02:06.\n",
      "  Batch   520  of    570.    Elapsed: 0:02:16.\n",
      "  Batch   560  of    570.    Elapsed: 0:02:27.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:02:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    570.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    570.    Elapsed: 0:00:21.\n",
      "  Batch   120  of    570.    Elapsed: 0:00:32.\n",
      "  Batch   160  of    570.    Elapsed: 0:00:43.\n",
      "  Batch   200  of    570.    Elapsed: 0:00:53.\n",
      "  Batch   240  of    570.    Elapsed: 0:01:04.\n",
      "  Batch   280  of    570.    Elapsed: 0:01:15.\n",
      "  Batch   320  of    570.    Elapsed: 0:01:25.\n",
      "  Batch   360  of    570.    Elapsed: 0:01:36.\n",
      "  Batch   400  of    570.    Elapsed: 0:01:47.\n",
      "  Batch   440  of    570.    Elapsed: 0:01:57.\n",
      "  Batch   480  of    570.    Elapsed: 0:02:08.\n",
      "  Batch   520  of    570.    Elapsed: 0:02:18.\n",
      "  Batch   560  of    570.    Elapsed: 0:02:29.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:02:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:06\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Total training took 0:05:29 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    if epoch_i >= 1:\n",
    "        inp = input()\n",
    "        if inp.startswith('y'):\n",
    "            break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:02.361998Z",
     "iopub.status.busy": "2020-10-30T00:15:02.361754Z",
     "iopub.status.idle": "2020-10-30T00:15:02.374864Z",
     "shell.execute_reply": "2020-10-30T00:15:02.374369Z",
     "shell.execute_reply.started": "2020-10-30T00:15:02.361973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0:02:29</td>\n",
       "      <td>0:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0:02:31</td>\n",
       "      <td>0:00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.70         0.69           0.44       0:02:29         0:00:06\n",
       "2               0.69         0.69           0.56       0:02:31         0:00:06"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\"\"\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:02.376098Z",
     "iopub.status.busy": "2020-10-30T00:15:02.375842Z",
     "iopub.status.idle": "2020-10-30T00:15:02.561583Z",
     "shell.execute_reply": "2020-10-30T00:15:02.561082Z",
     "shell.execute_reply.started": "2020-10-30T00:15:02.376055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:02.564105Z",
     "iopub.status.busy": "2020-10-30T00:15:02.563937Z",
     "iopub.status.idle": "2020-10-30T00:15:02.566593Z",
     "shell.execute_reply": "2020-10-30T00:15:02.566062Z",
     "shell.execute_reply.started": "2020-10-30T00:15:02.564084Z"
    }
   },
   "outputs": [],
   "source": [
    "posts = valid_x.values\n",
    "categories = valid_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:02.567761Z",
     "iopub.status.busy": "2020-10-30T00:15:02.567591Z",
     "iopub.status.idle": "2020-10-30T00:15:02.759994Z",
     "shell.execute_reply": "2020-10-30T00:15:02.759507Z",
     "shell.execute_reply.started": "2020-10-30T00:15:02.567740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:02.760942Z",
     "iopub.status.busy": "2020-10-30T00:15:02.760784Z",
     "iopub.status.idle": "2020-10-30T00:15:09.131127Z",
     "shell.execute_reply": "2020-10-30T00:15:09.130539Z",
     "shell.execute_reply.started": "2020-10-30T00:15:02.760922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,441 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:09.132189Z",
     "iopub.status.busy": "2020-10-30T00:15:09.131927Z",
     "iopub.status.idle": "2020-10-30T00:15:09.170304Z",
     "shell.execute_reply": "2020-10-30T00:15:09.169778Z",
     "shell.execute_reply.started": "2020-10-30T00:15:09.132165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n",
      "Accuracy: 0.5503122831367107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "predicts = []\n",
    "accurate = 0\n",
    "total_len = 0\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    predicts.append(pred_labels_i)\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "\n",
    "    matthews_set.append(matthews)\n",
    "    for j in range(len(true_labels[i])):\n",
    "        if true_labels[i][j] == pred_labels_i[j]:\n",
    "            accurate+=1\n",
    "        total_len+=1\n",
    "print(\"Accuracy:\",accurate/total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:09.171276Z",
     "iopub.status.busy": "2020-10-30T00:15:09.171118Z",
     "iopub.status.idle": "2020-10-30T00:15:09.174718Z",
     "shell.execute_reply": "2020-10-30T00:15:09.174186Z",
     "shell.execute_reply.started": "2020-10-30T00:15:09.171256Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T00:15:09.175803Z",
     "iopub.status.busy": "2020-10-30T00:15:09.175509Z",
     "iopub.status.idle": "2020-10-30T00:15:09.188294Z",
     "shell.execute_reply": "2020-10-30T00:15:09.187721Z",
     "shell.execute_reply.started": "2020-10-30T00:15:09.175780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       648\n",
      "           1       0.55      1.00      0.71       793\n",
      "\n",
      "    accuracy                           0.55      1441\n",
      "   macro avg       0.28      0.50      0.35      1441\n",
      "weighted avg       0.30      0.55      0.39      1441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
