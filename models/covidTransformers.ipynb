{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-f910fae0-ba10-4708-9049-670a93f637a9",
    "tags": []
   },
   "source": [
    "- Database: Covid-fake\n",
    "- Function: cleaning\n",
    "- Desp: NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-b7880f68-fb43-45dd-9963-39c76f34c960"
   },
   "source": [
    "# Necessary Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00002-9b8431e6-7762-485a-9cad-b826b8b2e453",
    "execution": {
     "iopub.status.idle": "2020-10-29T15:01:07.743121Z",
     "shell.execute_reply": "2020-10-29T15:01:07.742468Z",
     "shell.execute_reply.started": "2020-10-29T15:01:04.840434Z"
    },
    "execution_millis": 1610,
    "execution_start": 1602317375718,
    "output_cleared": false,
    "source_hash": "6a0e8856"
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:07.744318Z",
     "iopub.status.busy": "2020-10-29T15:01:07.744157Z",
     "iopub.status.idle": "2020-10-29T15:01:07.769970Z",
     "shell.execute_reply": "2020-10-29T15:01:07.769526Z",
     "shell.execute_reply.started": "2020-10-29T15:01:07.744298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:2\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:07.773101Z",
     "iopub.status.busy": "2020-10-29T15:01:07.772936Z",
     "iopub.status.idle": "2020-10-29T15:01:10.758547Z",
     "shell.execute_reply": "2020-10-29T15:01:10.758051Z",
     "shell.execute_reply.started": "2020-10-29T15:01:07.773082Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['bert-base-uncased', 'distilbert-base-uncased-finetuned-sst-2-english', 'textattack/roberta-base-SST-2','roberta-large', 'google/electra-base-discriminator', 'xlnet-base-cased', 'xlm-roberta-base']\n",
    "model_num = 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-7462cdd6-7996-40b9-b2ad-4f9db1c30a21"
   },
   "source": [
    "# Read test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00004-79ce4e19-951f-4d1d-b372-9315510c9e68",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.759573Z",
     "iopub.status.busy": "2020-10-29T15:01:10.759353Z",
     "iopub.status.idle": "2020-10-29T15:01:10.867148Z",
     "shell.execute_reply": "2020-10-29T15:01:10.866660Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.759552Z"
    },
    "execution_millis": 105,
    "execution_start": 1602317380627,
    "output_cleared": false,
    "source_hash": "9f16c2f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/covid/Constraint_English_Train - Sheet1.csv')\n",
    "test = pd.read_csv('../datasets/covid/Constraint_English_Val - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00005-806e1fa7-a81f-4332-9028-e656062a1e65",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.868078Z",
     "iopub.status.busy": "2020-10-29T15:01:10.867925Z",
     "iopub.status.idle": "2020-10-29T15:01:10.878387Z",
     "shell.execute_reply": "2020-10-29T15:01:10.877835Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.868058Z"
    },
    "execution_millis": 104,
    "execution_start": 1602317385054,
    "output_cleared": false,
    "source_hash": "2ecaabe6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Covid Act Now found \"on average each person in...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>If you tested positive for #COVID19 and have n...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Obama Calls Trump’s Coronavirus Response A Cha...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>???Clearly, the Obama administration did not l...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Retraction—Hydroxychloroquine or chloroquine w...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real\n",
       "5   6  Covid Act Now found \"on average each person in...  real\n",
       "6   7  If you tested positive for #COVID19 and have n...  real\n",
       "7   8  Obama Calls Trump’s Coronavirus Response A Cha...  fake\n",
       "8   9  ???Clearly, the Obama administration did not l...  fake\n",
       "9  10  Retraction—Hydroxychloroquine or chloroquine w...  fake"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00006-5229c43c-176e-4e28-99ae-4fa40154c8ba",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.879208Z",
     "iopub.status.busy": "2020-10-29T15:01:10.879071Z",
     "iopub.status.idle": "2020-10-29T15:01:10.882112Z",
     "shell.execute_reply": "2020-10-29T15:01:10.881514Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.879190Z"
    },
    "execution_millis": 2,
    "execution_start": 1602317390647,
    "output_cleared": false,
    "source_hash": "3300ffa3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['fake','real']\n",
    "def label_encode(val):\n",
    "    return labels.index(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-4e0c86f9-360f-44bb-a346-33a9f60bfa34"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00008-d2221d9d-13a4-4124-bcc1-f8e296b9025b",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.883075Z",
     "iopub.status.busy": "2020-10-29T15:01:10.882899Z",
     "iopub.status.idle": "2020-10-29T15:01:10.890766Z",
     "shell.execute_reply": "2020-10-29T15:01:10.890322Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.883056Z"
    },
    "execution_millis": 29,
    "execution_start": 1602317392478,
    "output_cleared": false,
    "source_hash": "4a70d54d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.label = train.label.apply(label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00009-27915fb6-5377-4252-b54d-f5ab82228487",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.892509Z",
     "iopub.status.busy": "2020-10-29T15:01:10.892358Z",
     "iopub.status.idle": "2020-10-29T15:01:10.901418Z",
     "shell.execute_reply": "2020-10-29T15:01:10.900978Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.892491Z"
    },
    "execution_millis": 14,
    "execution_start": 1602317413798,
    "output_cleared": false,
    "source_hash": "2ecaabe6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Covid Act Now found \"on average each person in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>If you tested positive for #COVID19 and have n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Obama Calls Trump’s Coronavirus Response A Cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>???Clearly, the Obama administration did not l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Retraction—Hydroxychloroquine or chloroquine w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...      1\n",
       "1   2  States reported 1121 deaths a small rise from ...      1\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...      0\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...      1\n",
       "4   5  Populous states can generate large case counts...      1\n",
       "5   6  Covid Act Now found \"on average each person in...      1\n",
       "6   7  If you tested positive for #COVID19 and have n...      1\n",
       "7   8  Obama Calls Trump’s Coronavirus Response A Cha...      0\n",
       "8   9  ???Clearly, the Obama administration did not l...      0\n",
       "9  10  Retraction—Hydroxychloroquine or chloroquine w...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-d9da0c39-f1ea-46d4-9715-9cea25c13fc8"
   },
   "source": [
    "# Cleaning training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00011-ef6a747a-1158-4ea4-86ed-def67676f4fb",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:10.902772Z",
     "iopub.status.busy": "2020-10-29T15:01:10.902619Z",
     "iopub.status.idle": "2020-10-29T15:01:11.004518Z",
     "shell.execute_reply": "2020-10-29T15:01:11.004047Z",
     "shell.execute_reply.started": "2020-10-29T15:01:10.902752Z"
    },
    "execution_millis": 339,
    "execution_start": 1602317420342,
    "output_cleared": false,
    "source_hash": "a8298672",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "train.tweet = train.tweet.apply(clean_text)\n",
    "train.tweet = train.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-da3221db-7d57-4404-8eaf-c8f10ac4c499"
   },
   "source": [
    "### Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00013-c8cdfe1d-5b09-41d3-8f6b-a7fa5f75ec3b",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.005446Z",
     "iopub.status.busy": "2020-10-29T15:01:11.005291Z",
     "iopub.status.idle": "2020-10-29T15:01:11.040856Z",
     "shell.execute_reply": "2020-10-29T15:01:11.040381Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.005426Z"
    },
    "execution_millis": 164,
    "execution_start": 1602317421233,
    "output_cleared": false,
    "source_hash": "21ef7253"
   },
   "outputs": [],
   "source": [
    "test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "test.tweet = test.tweet.apply(clean_text)\n",
    "test.tweet = test.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00014-05bed7fb-4f79-4c70-955c-89dfa07c2a32",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.041682Z",
     "iopub.status.busy": "2020-10-29T15:01:11.041538Z",
     "iopub.status.idle": "2020-10-29T15:01:11.045887Z",
     "shell.execute_reply": "2020-10-29T15:01:11.045433Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.041663Z"
    },
    "execution_millis": 6,
    "execution_start": 1602317425170,
    "output_cleared": false,
    "source_hash": "81695e2c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950    we still have  significant clusters four of wh...\n",
       "2747    in the last one week weve trained more lab sci...\n",
       "714     our daily update is published weve now tracked...\n",
       "900     coronavirus only one in  to be protected from ...\n",
       "664     rt drharshvardhan #covid update a total of  ne...\n",
       "3601    covid update there are no new cases of covid t...\n",
       "4936    employees at home working hard at pretending t...\n",
       "1598    the souths sevenday average for new cases per ...\n",
       "4851    says a photo shows the same exact people at tw...\n",
       "2166    a picture shows an eightyearold boy infected w...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tweet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00019-c6184073-f974-467e-8520-bd6764ca5d22",
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.046720Z",
     "iopub.status.busy": "2020-10-29T15:01:11.046578Z",
     "iopub.status.idle": "2020-10-29T15:01:11.051390Z",
     "shell.execute_reply": "2020-10-29T15:01:11.050846Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.046702Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['tweet'], train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.052245Z",
     "iopub.status.busy": "2020-10-29T15:01:11.052100Z",
     "iopub.status.idle": "2020-10-29T15:01:11.055849Z",
     "shell.execute_reply": "2020-10-29T15:01:11.055338Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.052226Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.056675Z",
     "iopub.status.busy": "2020-10-29T15:01:11.056534Z",
     "iopub.status.idle": "2020-10-29T15:01:11.071302Z",
     "shell.execute_reply": "2020-10-29T15:01:11.070858Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.056656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.41140965732087"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in train_x:\n",
    "    total += count_words(i)\n",
    "total/len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.072226Z",
     "iopub.status.busy": "2020-10-29T15:01:11.072077Z",
     "iopub.status.idle": "2020-10-29T15:01:11.076739Z",
     "shell.execute_reply": "2020-10-29T15:01:11.076256Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.072207Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "posts = train_x.values\n",
    "categories = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.077626Z",
     "iopub.status.busy": "2020-10-29T15:01:11.077482Z",
     "iopub.status.idle": "2020-10-29T15:01:11.089479Z",
     "shell.execute_reply": "2020-10-29T15:01:11.088974Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.077607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  4825,   368,  4347,  2318,   910,   261,  2694, 22122, 14084,\n",
       "          5559, 47268,   808,  1263,    30, 13241,  9240,  4488,  4437,    11,\n",
       "           645,     7, 29857,   225, 23387, 14414,  9158,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        posts[2],                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:11.090495Z",
     "iopub.status.busy": "2020-10-29T15:01:11.090346Z",
     "iopub.status.idle": "2020-10-29T15:01:13.309616Z",
     "shell.execute_reply": "2020-10-29T15:01:13.308956Z",
     "shell.execute_reply.started": "2020-10-29T15:01:11.090476Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:13.310892Z",
     "iopub.status.busy": "2020-10-29T15:01:13.310717Z",
     "iopub.status.idle": "2020-10-29T15:01:13.316142Z",
     "shell.execute_reply": "2020-10-29T15:01:13.315649Z",
     "shell.execute_reply.started": "2020-10-29T15:01:13.310869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,494 training samples\n",
      "  642 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.875 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:13.317017Z",
     "iopub.status.busy": "2020-10-29T15:01:13.316879Z",
     "iopub.status.idle": "2020-10-29T15:01:13.323113Z",
     "shell.execute_reply": "2020-10-29T15:01:13.322580Z",
     "shell.execute_reply.started": "2020-10-29T15:01:13.316999Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:13.323915Z",
     "iopub.status.busy": "2020-10-29T15:01:13.323773Z",
     "iopub.status.idle": "2020-10-29T15:01:32.314714Z",
     "shell.execute_reply": "2020-10-29T15:01:32.314176Z",
     "shell.execute_reply.started": "2020-10-29T15:01:13.323897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.\n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.315888Z",
     "iopub.status.busy": "2020-10-29T15:01:32.315719Z",
     "iopub.status.idle": "2020-10-29T15:01:32.324256Z",
     "shell.execute_reply": "2020-10-29T15:01:32.323844Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.315865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 395 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50265, 1024)\n",
      "roberta.embeddings.position_embeddings.weight            (514, 1024)\n",
      "roberta.embeddings.token_type_embeddings.weight            (1, 1024)\n",
      "roberta.embeddings.LayerNorm.weight                          (1024,)\n",
      "roberta.embeddings.LayerNorm.bias                            (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.query.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight       (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.key.bias              (1024,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.value.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight   (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias          (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight      (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias      (1024,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight       (4096, 1024)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (4096,)\n",
      "roberta.encoder.layer.0.output.dense.weight             (1024, 4096)\n",
      "roberta.encoder.layer.0.output.dense.bias                    (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight              (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                 (1024, 1024)\n",
      "classifier.dense.bias                                        (1024,)\n",
      "classifier.out_proj.weight                                 (2, 1024)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.325210Z",
     "iopub.status.busy": "2020-10-29T15:01:32.325055Z",
     "iopub.status.idle": "2020-10-29T15:01:32.332074Z",
     "shell.execute_reply": "2020-10-29T15:01:32.331616Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.325190Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.332958Z",
     "iopub.status.busy": "2020-10-29T15:01:32.332796Z",
     "iopub.status.idle": "2020-10-29T15:01:32.341011Z",
     "shell.execute_reply": "2020-10-29T15:01:32.340551Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.332938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 10\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.341886Z",
     "iopub.status.busy": "2020-10-29T15:01:32.341733Z",
     "iopub.status.idle": "2020-10-29T15:01:32.348061Z",
     "shell.execute_reply": "2020-10-29T15:01:32.347594Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.341867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.348954Z",
     "iopub.status.busy": "2020-10-29T15:01:32.348801Z",
     "iopub.status.idle": "2020-10-29T15:01:32.354363Z",
     "shell.execute_reply": "2020-10-29T15:01:32.353909Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.348934Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:01:32.356645Z",
     "iopub.status.busy": "2020-10-29T15:01:32.356493Z",
     "iopub.status.idle": "2020-10-29T15:14:01.342403Z",
     "shell.execute_reply": "2020-10-29T15:14:01.341674Z",
     "shell.execute_reply.started": "2020-10-29T15:01:32.356626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:20.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:40.\n",
      "  Batch   120  of    141.    Elapsed: 0:00:59.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epcoh took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.42\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:20.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:40.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:01.\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epcoh took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 0.70\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.54\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.60\n",
      "  Training epcoh took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 0.70\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 0.70\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    141.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    141.    Elapsed: 0:01:02.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:12:29 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        outputs = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(b_input_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "#     if epoch_i >= 1:\n",
    "#         inp = input()\n",
    "#         if inp.startswith('y'):\n",
    "#             break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:01.344035Z",
     "iopub.status.busy": "2020-10-29T15:14:01.343803Z",
     "iopub.status.idle": "2020-10-29T15:14:01.359038Z",
     "shell.execute_reply": "2020-10-29T15:14:01.358481Z",
     "shell.execute_reply.started": "2020-10-29T15:14:01.343979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0:01:10</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0:01:11</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0:01:13</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0:01:12</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.50         0.42           0.85       0:01:10         0:00:03\n",
       "2               0.48         0.70           0.52       0:01:11         0:00:03\n",
       "3               0.67         0.76           0.54       0:01:12         0:00:03\n",
       "4               0.60         0.68           0.52       0:01:13         0:00:03\n",
       "5               0.69         0.69           0.52       0:01:12         0:00:03\n",
       "6               0.70         0.70           0.50       0:01:12         0:00:03\n",
       "7               0.53         0.70           0.48       0:01:12         0:00:03\n",
       "8               0.70         0.69           0.48       0:01:12         0:00:03\n",
       "9               0.69         0.69           0.48       0:01:12         0:00:03\n",
       "10              0.69         0.69           0.48       0:01:12         0:00:03"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\"\"\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:01.360104Z",
     "iopub.status.busy": "2020-10-29T15:14:01.359870Z",
     "iopub.status.idle": "2020-10-29T15:14:01.612640Z",
     "shell.execute_reply": "2020-10-29T15:14:01.612208Z",
     "shell.execute_reply.started": "2020-10-29T15:14:01.360083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:01.613642Z",
     "iopub.status.busy": "2020-10-29T15:14:01.613453Z",
     "iopub.status.idle": "2020-10-29T15:14:01.616297Z",
     "shell.execute_reply": "2020-10-29T15:14:01.615835Z",
     "shell.execute_reply.started": "2020-10-29T15:14:01.613619Z"
    }
   },
   "outputs": [],
   "source": [
    "posts = valid_x.values\n",
    "categories = valid_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:01.617385Z",
     "iopub.status.busy": "2020-10-29T15:14:01.617207Z",
     "iopub.status.idle": "2020-10-29T15:14:02.115169Z",
     "shell.execute_reply": "2020-10-29T15:14:02.114578Z",
     "shell.execute_reply.started": "2020-10-29T15:14:01.617363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:02.116281Z",
     "iopub.status.busy": "2020-10-29T15:14:02.116064Z",
     "iopub.status.idle": "2020-10-29T15:14:08.030432Z",
     "shell.execute_reply": "2020-10-29T15:14:08.029759Z",
     "shell.execute_reply.started": "2020-10-29T15:14:02.116257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,284 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:08.031412Z",
     "iopub.status.busy": "2020-10-29T15:14:08.031188Z",
     "iopub.status.idle": "2020-10-29T15:14:08.034658Z",
     "shell.execute_reply": "2020-10-29T15:14:08.034173Z",
     "shell.execute_reply.started": "2020-10-29T15:14:08.031390Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:14:08.035761Z",
     "iopub.status.busy": "2020-10-29T15:14:08.035505Z",
     "iopub.status.idle": "2020-10-29T15:14:08.057231Z",
     "shell.execute_reply": "2020-10-29T15:14:08.056641Z",
     "shell.execute_reply.started": "2020-10-29T15:14:08.035722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       631\n",
      "           1       0.51      1.00      0.67       653\n",
      "\n",
      "    accuracy                           0.51      1284\n",
      "   macro avg       0.25      0.50      0.34      1284\n",
      "weighted avg       0.26      0.51      0.34      1284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "083d58b7-ade2-4beb-9a11-d89206177ed2",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
